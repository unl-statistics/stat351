{
  "hash": "760357994d84bd1fefe04d913511a332",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Why Arrow?\"\nformat: \n    revealjs:\n        navigation-mode: vertical\n        logo: deps/unl/N.svg\n        theme: deps/unl/inverse.scss\n        css: deps/unl/styles.css\n        includes:\n          in_header: deps/unl/header.html\n        lib_dir: libs\nlightbox: true\n---\n\n## Overview\n\n<style>\n.extrapad {\n    margin-top:1em  !important;\n}\n</style>\n\n1. **Introduction to Column-Oriented Data Storage**\n   <!-- - The big data interchange problem\n   - Row vs. column storage formats\n   - Introduction to Parquet and Arrow -->\n\n2. **Deep Dive into Parquet** \n   <!-- - Key features and benefits\n   - Performance benchmarks\n   - Demo: Efficient dataset storage -->\n\n3. **Working with Arrow in R** \n   <!-- - The Arrow package\n   - Reading/writing Parquet files\n   - Using dplyr with Arrow tables -->\n\n4. **Querying Parquet with Different Engines** \n   <!-- - DuckDB: SQL queries on Parquet files\n   - Data.table approach\n   - Arrow query execution\n   - Performance comparisons -->\n\n5. **Arrow Datasets for Larger-than-Memory Operations** \n   <!-- - Datasets vs. Tables\n   - Handling data too large for memory\n   - Working with datasets on S3 -->\n\n6. **Partitioning Strategies** \n   <!-- - Understanding partitioning\n   - Hive vs. non-Hive partitioning\n   - Best practices -->\n\n7. **Hands-on Workshop: Analysis with PUMS Data** \n   <!-- - Reading partitioned Parquet files\n   - Executing queries with DuckDB and Arrow\n   - Practice exercises -->\n\n[These slides borrowed from the [JSM Big Data 2025 workshop](https://github.com/kbodwin/jsm-big-data-2025)]{.footnote}\n\n# Introduction to Column-Oriented Data Storage\n\n## Data storage?\n\n::: {.fragment .extrapad}\nData has to be represented somewhere, both during analysis and when storing.\n:::\n\n::: {.fragment .extrapad}\nThe shape and characteristics of this representation impacts performance.\n:::\n\n::: {.fragment .extrapad}\nWhat if you could speed up a key part of your analysis by 30x and reduce your storage by 10x?\n:::\n\n## Row vs. Column-Oriented Storage\n\n::: columns\n::: {.column width=\"50%\"}\n**Row-oriented**\n\n```\n|ID|Name |Age|City    |\n|--|-----|---|--------|\n|1 |Alice|25 |New York|\n|2 |Bob  |30 |Boston  |\n|3 |Carol|45 |Chicago |\n```\n\n- Efficient for single record access\n- Efficient for appending\n:::\n\n::: {.column width=\"50%\"}\n::: {.fragment}\n**Column-oriented**\n\n```\nID:    [1, 2, 3]\nName:  [Alice, Bob, Carol]\nAge:   [25, 30, 45]\nCity:  [New York, Boston, Chicago]\n```\n\n- Efficient for analytics\n- Better compression\n:::\n:::\n:::\n\n::: {.notes}\nRow oriented formats are super familiar: CSVs as well as many databases\n\nBut Column-orientation isn't something that is new and cutting edge. In fact, every single one of you use a system that stores data this way: R data frames(!)\n:::\n\n## Why Column-Oriented Storage?\n\n::: {.incremental}\n- **Analytics typically access a subset of columns**\n  - \"What is the average age by city?\"\n  - Only needs [Age, City] columns\n\n- **Benefits:**\n  - Only read needed columns from disk\n  - Similar data types stored together\n  - Better compression ratios\n:::\n\n::: {.notes}\nCompression: this is because like-types are stored with like, so you get more frequent patterns — the core of compression. But you also can use encodings like dictionary encodings very efficiently.\n:::\n\n## Column-Oriented Data is great \n\n:::{.extrapad}\nAnd you use column-oriented dataframes already!\n:::\n\n::: {.fragment}\n... but still storing my data in a fundamentally row-oriented way. \n:::\n\n::: {.notes}\nThis isn't so bad if you're only talking about a small amount of data, transposing a few columns for a few rows is no big deal. But as data gets larger, or if you have to do this frequently, this process of transposing (AKA serialization) hurts.\n:::\n\n## The interconnection problem\n\n![](images/copy-convert.png){.r-stretch}\n\n::: {.notes}\nMany of these were operating in essentially column-oriented ways — but to transfer data ended up writting into row-oriented data structures, then read them back in to something that was column-oriented.\n\n**Moving data between representations is hard**\n  - Different formats, requirements, and limitations\n  - Compatibility issues\n  - Serialization is a huge bottleneck\n:::\n\n## The interconnection problem\n\n![](images/shared.png){.r-stretch}\n\n## What is Apache Arrow?\n\n::: columns\n::: {.column width=\"20%\"}\n![](images/arrow-logo_vertical_black-txt_transparent-bg.png)\n:::\n::: {.column width=\"80%\"}\n::: {.incremental}\n- **Cross-language development platform for in-memory data**\n  - Consistent in-memory columnar data format\n  - Language-independent\n  - Zero-copy reads\n\n- **Benefits:**\n  - Seamless data interchange between systems\n  - Fast analytical processing\n  - Efficient memory usage\n:::\n:::\n:::\n\n## What is Apache Parquet?\n\n![](images/Apache_Parquet_logo.svg){width=40%}\n\n::: {.incremental}\n- **Open-source columnar storage format**\n  - Created by Twitter and Cloudera in 2013\n  - Part of the Apache Software Foundation\n\n- **Features:**\n  - Columnar storage\n  - Efficient compression\n  - Explicit schema \n  - Statistical metadata\n:::\n\n## Get the Data\n\n[Download the data](https://github.com/arrowrbook/book/releases/download/PUMS_subset/PUMS.subset.zip)\n\nhttps://github.com/arrowrbook/book/releases/download/PUMS_subset/PUMS.subset.zip\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](15-why-arrow_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n## Get the Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(timeout = max(300, getOption(\"timeout\")))\n\nif(!file.exists(\"data/PUMS.subset.zip\")) {\n  download.file(\"https://github.com/arrowrbook/book/releases/download/PUMS_subset/PUMS.subset.zip\", destfile=\"data/PUMS.subset.zip\", cacheok = T)\n}\n\nunzip(\"data/PUMS.subset.zip\", exdir = \"data\", overwrite = T)\n```\n:::\n\n\n\n## Reading a File\n\n\nAs a CSV file\n\n\n::: {.cell}\n\n:::\n\n:::{.fragment .extrapad}\n```\n   user  system elapsed \n  2.278   0.048   2.347\n```\n:::\n\n## Reading a File\n\nAs a zipped CSV file\n\n\n::: {.cell}\n\n:::\n\n:::{.fragment .extrapad}\n```\n   user  system elapsed \n  0.268   0.000   0.293 \n```\n:::\n\n\n## Reading a File\n\nAs a CSV file with arrow\n\n\n::: {.cell}\n\n:::\n\n:::{.fragment .extrapad}\n```\n   user  system elapsed \n  0.931   0.277   0.196 \n```\n:::\n\n## Reading a File\n\nAs a Parquet file\n\n\n::: {.cell}\n\n:::\n\n:::{.fragment .extrapad}\n```\n  user  system elapsed \n  0.105   0.015   0.120 \n```\n:::\n\n:::{.notes}\nThe parquet file is 12 MB\n\nIt's even faster with altrep (0.186 s), but that's cheating! Also, if we read into an arrow table rather than a dataframe: 0.1 second \n:::\n\n# Deep Dive into Parquet\n\n## What is Parquet?\n\n::: {.incremental}\n- **Schema metadata**\n  - Self-describing format\n  - Preserves column types\n  - Type-safe data interchange\n\n- **Encodings**\n  - **Dictionary** — Particularly effective for categorical data\n  - **Run-length encoding** - Efficient storage of sequential repeated values\n\n- **Advanced compression**\n  - Column-specific compression algorithms\n  - Both dictionary and value compression\n:::\n\n## Exercise\n\n\n::: {.cell}\n\n:::\n\n\n::: {.fragment .extrapad}\nAre there any differences?\n:::\n\n## Exercise\n\n:::{.columns}\n:::{.column}\n```{.default}\n> df_csv_arrow\n# A tibble: 10 × 3\n   integers doubles strings\n      <int>   <int>   <int>\n 1        1       1       1\n 2        2       2       2\n 3        3       3       3\n 4        4       4       4\n 5        5       5       5\n 6        6       6       6\n 7        7       7       7\n 8        8       8       8\n 9        9       9       9\n10       10      10      10\n```\n:::\n\n:::{.column}\n```{.default}\n> df_parquet\n# A tibble: 10 × 3\n   integers doubles strings\n      <int>   <dbl> <chr>  \n 1        1       1 01     \n 2        2       2 02     \n 3        3       3 03     \n 4        4       4 04     \n 5        5       5 05     \n 6        6       6 06     \n 7        7       7 07     \n 8        8       8 08     \n 9        9       9 09     \n10       10      10 10     \n```\n:::\n:::\n\n## Exercise\n\n:::{.columns}\n:::{.column}\n```{.default code-line-numbers=\"3-4\"}\n> df_csv_arrow\n# A tibble: 10 × 3\n   integers doubles strings\n      <int>   <int>   <int>\n 1        1       1       1\n 2        2       2       2\n 3        3       3       3\n 4        4       4       4\n 5        5       5       5\n 6        6       6       6\n 7        7       7       7\n 8        8       8       8\n 9        9       9       9\n10       10      10      10\n```\n:::\n\n:::{.column}\n```{.default code-line-numbers=\"3-4\"}\n> df_parquet\n# A tibble: 10 × 3\n   integers doubles strings\n      <int>   <dbl> <chr>  \n 1        1       1 01     \n 2        2       2 02     \n 3        3       3 03     \n 4        4       4 04     \n 5        5       5 05     \n 6        6       6 06     \n 7        7       7 07     \n 8        8       8 08     \n 9        9       9 09     \n10       10      10 10     \n```\n:::\n:::\n\n## Inside a Parquet File\n\n![](images/files_formats_parquet_bw.png){width=100%}\n\n::: {.notes}\n- **Row groups**: Horizontal partitions of data\n- **Column chunks**: Columnar data within a row group\n- **Pages**: Small units of column chunk data\n- **Footer**: Contains file metadata and schema\n:::\n\n## Benchmarks: Parquet vs CSV\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](15-why-arrow_files/figure-revealjs/unnamed-chunk-9-1.png){width=408}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](15-why-arrow_files/figure-revealjs/unnamed-chunk-10-1.png){width=408}\n:::\n:::\n\n:::\n:::\n\n## Reading Efficiency: Selecting Columns\n\n::: {.incremental}\n- **With CSV:**\n  - Must read entire file, even if you only need a few columns\n  - No efficient way to skip columns during read\n\n- **With Parquet:**\n  - Read only needed columns from disk\n  - Significant performance benefit for wide tables\n\n::: {.fragment}\n\n::: {.cell}\n\n:::\n\n\n:::\n\n::: {.columns}\n::: {.column .fragment}\n```\n   user  system elapsed \n  0.012   0.000   0.011\n```\n:::\n::: {.column .fragment}\n```\n   user  system elapsed \n  0.301   0.053   0.084 \n```\n:::\n:::\n:::\n\n## Parquet Tooling Ecosystem\n\n**Languages with native Parquet support:**\n\n::: {.incremental}\n- R (via arrow)\n- Python (via pyarrow, pandas)\n- Java\n- C++\n- Rust\n- JavaScript\n- Go\n:::\n\n## Parquet Tooling Ecosystem\n\n**Systems with Parquet integration:**\n\n::: {.incremental}\n- Apache Spark\n- Apache Hadoop\n- Apache Drill\n- Snowflake\n- Amazon Athena\n- Google BigQuery\n- DuckDB\n:::\n\n# Working with Parquet files with Arrow in R\n\n## Introduction to the arrow Package\n\n\n::: {.cell}\n\n:::\n\n\n::: {.incremental}\n- The **arrow** package provides:\n  - Native R interface to Apache Arrow\n  - Tools for working with large datasets\n  - Integration with dplyr for data manipulation\n  - Reading/writing various file formats\n:::\n\n## Reading and Writing Parquet files, revisited\n\n\n::: {.cell}\n\n:::\n\n\n## Demo: Using dplyr with arrow\n\n\n::: {.cell}\n\n:::\n\n\n::: {.notes}\nThe dataframe is backed by altrep, actually. But generally functions like any other dataframe.\n:::\n\n# Arrow Datasets for Larger-than-Memory Operations\n\n## Understanding Arrow Datasets vs. Tables\n\n::: columns\n::: {.column width=\"50%\"}\n**Arrow Table**\n\n- In-memory data structure\n- Must fit in RAM\n- Fast operations\n- Similar to base data frames\n- Good for single file data\n:::\n\n::: {.column width=\"50%\"}\n**Arrow Dataset**\n\n- Collection of files\n- Lazily evaluated\n- Larger-than-memory capable\n- Distributed execution\n- Supports partitioning\n:::\n:::\n\n## Demo: Opening and Querying Multi-file Datasets\n\n\n::: {.cell}\n\n:::\n\n\n## Lazy Evaluation and Query Optimization\n\n::: {.incremental}\n- **Lazy evaluation workflow:**\n  1. Define operations (filter, group, summarize)\n  2. Arrow builds an execution plan\n  3. Optimizes the plan (predicate pushdown, etc.)\n  4. Only reads necessary data from disk\n  5. Executes when `collect()` is called\n\n- **Benefits:**\n  - Minimizes memory usage\n  - Reduces I/O operations\n  - Leverages Arrow's native compute functions\n:::\n\n# Partitioning Strategies\n\n## What is Partitioning?\n\n::: {.incremental}\n- **Dividing data into logical segments**\n  - Stored in separate files/directories\n  - Based on one or more column values\n  - Enables efficient filtering\n\n- **Benefits:**\n  - Faster queries that filter on partition columns\n  - Improved parallel processing\n  - Easier management of large datasets\n:::\n\n![](https://arrow.apache.org/docs/r/_images/dataset-parquet-partition.svg){width=50%}\n\n## Hive vs. Non-Hive Partitioning\n\n::: columns\n::: {.column width=\"50%\"}\n**Hive Partitioning**\n\n- Directory format: `column=value`\n- Example:\n  ```\n  person/\n  ├── year=2018/\n  │   ├── state=NY/\n  │   │   └── data.parquet\n  │   └── state=CA/\n  │       └── data.parquet\n  ├── year=2019/\n  │   ├── ...\n  ```\n- Self-describing structure\n- Standard in big data ecosystem\n:::\n\n::: {.column width=\"50%\"}\n**Non-Hive Partitioning**\n\n- Directory format: `value`\n- Example:\n  ```\n  person/\n  ├── 2018/\n  │   ├── NY/\n  │   │   └── data.parquet\n  │   └── CA/\n  │       └── data.parquet\n  ├── 2019/\n  │   ├── ...\n  ```\n- Requires column naming\n- Less verbose directory names\n:::\n:::\n\n## Effective Partitioning Strategies\n\n::: {.incremental}\n- **Choose partition columns wisely:**\n  - Low to medium number of objects\n  - Commonly used in filters\n  - Balanced data distribution\n\n- **Common partition dimensions:**\n  - Time (year, month, day)\n  - Geography (country, state, region)\n  - Category (product type, department)\n  - Source (system, sensor)\n:::\n\n## Partitioning in Practice: Writing Datasets\n\n\n::: {.cell}\n\n:::\n\n\n## Demo: repartitioning the whole dataset\n\n::: {.cell}\n\n:::\n\n\n## Best Practices for Partition Design\n\n::: {.incremental}\n- **Avoid over-partitioning:**\n  - Too many small files = poor performance\n  - Target file size: 20MB–2GB\n  - Avoid high-cardinality columns (e.g., user_id)\n\n- **Consider query patterns:**\n  - Partition by commonly filtered columns\n  - Balance between read speed and write complexity\n\n- **Nested partitioning considerations:**\n  - Order from highest to lowest selectivity\n  - Limit partition depth (2-3 levels typically sufficient)\n:::\n\n## Partitioning Performance Impact\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](15-why-arrow_files/figure-revealjs/unnamed-chunk-19-1.png){width=768}\n:::\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Conclusion\n\n:::{.incremental}\n- Column-oriented storage formats like Parquet provide massive \nperformance advantages for analytical workloads (30x speed, 10x smaller\nfiles)\n- Apache Arrow enables seamless data interchange between systems without\ncostly serialization/deserialization\n- Partitioning strategies help manage large datasets effectively when\nworking with data too big for memory\n:::\n\n## Conclusion\n\n**Resources:**\n\n- Arrow documentation: [arrow.apache.org/docs/r](https://arrow.apache.org/docs/r/)\n- Parquet: [parquet.apache.org](https://parquet.apache.org/)\n- DuckDB: [duckdb.org](https://duckdb.org/)\n- Book: [Scaling up with Arrow and R](https://arrow-user2022.github.io/scaling-r-with-arrow/)\n",
    "supporting": [
      "15-why-arrow_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}