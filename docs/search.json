[
  {
    "objectID": "slides/01-02-html-xml-primer.html#why",
    "href": "slides/01-02-html-xml-primer.html#why",
    "title": "HTML, XML, CSS, and XPath",
    "section": "Why?",
    "text": "Why?\n\n\nSo much data is online!\nBut, …\n\nManually copying information is error prone\nStuff is updated in real time\nI’m lazy\n\nWe can automate gathering data using scripts"
  },
  {
    "objectID": "slides/01-02-html-xml-primer.html#examples",
    "href": "slides/01-02-html-xml-primer.html#examples",
    "title": "HTML, XML, CSS, and XPath",
    "section": "Examples",
    "text": "Examples\n\nGather the price of something every morning at 8am to determine whether it’s cheap enough to buy\nEstimate what you could sell your house for, given recent sales of comparable houses in the same city\nCreate a database of adoptable pets in your area (with pics) and determine how long it takes for different pets to be adopted\nAssemble a database of profiles on dating sites to determine whether gender and sexual orientation is related to emoticon/emoji use\nAssemble a directory of contact information for all faculty at UNL to conduct an unofficial faculty survey"
  },
  {
    "objectID": "slides/01-02-html-xml-primer.html#xml",
    "href": "slides/01-02-html-xml-primer.html#xml",
    "title": "HTML, XML, CSS, and XPath",
    "section": "XML",
    "text": "XML\n\nrelatively common data storage format\nFields are delimited by tags &lt;tagName attr1=value1 ...&gt;. All tags are closed with &lt;/tagName&gt;\nTags may contain attribute-value pairs\nTags may have children nested between &lt;tagName&gt; and &lt;/tagName&gt;\nTags may also contain “content” between the tags – plain text information"
  },
  {
    "objectID": "slides/01-02-html-xml-primer.html#xml-terms",
    "href": "slides/01-02-html-xml-primer.html#xml-terms",
    "title": "HTML, XML, CSS, and XPath",
    "section": "XML Terms",
    "text": "XML Terms\n&lt;family name=\"Vanderplas\"&gt;\n    &lt;person given-name=\"Susan\"&gt;Mother&lt;/person&gt;\n    &lt;person given-name=\"Ryan\"&gt;Father&lt;/person&gt;\n    &lt;person given-name=\"Alex\" nickname='Bug'&gt;Son&lt;/person&gt;\n    &lt;person given-name=\"Zoey\" nickname='Zozo, Lovebug'&gt;Daughter&lt;/person&gt;\n    &lt;pet type=\"dog\" given-name=\"Edison\" nickname=\"Eddie\"&gt;Security detail, Cleanup crew&lt;/pet&gt;\n    &lt;pet type=\"dog\" given-name=\"Ivy\" nickname=\"Flufferina, Q-tip\"&gt;Snuggle agent, Cleanup crew, Comic relief&lt;/pet&gt;\n&lt;/family&gt;\n\ngiven-name and family-name are attributes with values for each person and pet. nickname is an attribute, but is not present for all nodes\n&lt;person&gt;...&lt;/person&gt; and &lt;pet&gt;...&lt;/pet&gt; are child nodes of &lt;family&gt;&lt;/family&gt;\nThe content of each child node is the entity’s role in the family\nXML data is nested and does not always translate to tabular form easily"
  },
  {
    "objectID": "slides/01-02-html-xml-primer.html#html-vs.-xml",
    "href": "slides/01-02-html-xml-primer.html#html-vs.-xml",
    "title": "HTML, XML, CSS, and XPath",
    "section": "HTML vs. XML",
    "text": "HTML vs. XML\n\n\n\nTags display information\nTags are pre-defined\nTags aren’t always closed\n&lt;br/&gt;, &lt;img/&gt;\nNot case-sensitive\nIgnores white-space\n\n\n\nTags describe information\nData schema defines tags\nTags must be closed\n \nCase-sensitive\nMay ignore white space"
  },
  {
    "objectID": "slides/01-02-html-xml-primer.html#your-turn-web-page-anatomy",
    "href": "slides/01-02-html-xml-primer.html#your-turn-web-page-anatomy",
    "title": "HTML, XML, CSS, and XPath",
    "section": "Your Turn: Web Page Anatomy",
    "text": "Your Turn: Web Page Anatomy\n\nOpen the textbook chapter\nAccess Developer Tools for your browser\n\nright-click + select “Inspect”\nOR, Ctrl/Cmd + J\n\nFind the following elements. What attributes and content do they have?\n\nDocument type declaration\n&lt;html&gt; node\n&lt;head&gt; and &lt;body&gt; nodes\n&lt;h2&gt;, &lt;h3&gt;, &lt;h4&gt; and &lt;p&gt; nodes\n&lt;table&gt;, &lt;tr&gt;, &lt;th&gt;, and &lt;td&gt; nodes\n&lt;a&gt; node(s)"
  },
  {
    "objectID": "slides/01-02-html-xml-primer.html#selecting-nodes-css",
    "href": "slides/01-02-html-xml-primer.html#selecting-nodes-css",
    "title": "HTML, XML, CSS, and XPath",
    "section": "Selecting Nodes (CSS)",
    "text": "Selecting Nodes (CSS)\n\nSelectorGadget extension can be helpful\n.xxx = “has class xxx”\n#xxx = “has ID xxx”\nxxx = “node xxx”\nxxx yyy = “node yyy, a descendant of xxx”\nxxx &gt; yyy = “node yyy, a direct descendant of xxx”"
  },
  {
    "objectID": "slides/01-02-html-xml-primer.html#your-turn-css-selectors",
    "href": "slides/01-02-html-xml-primer.html#your-turn-css-selectors",
    "title": "HTML, XML, CSS, and XPath",
    "section": "Your Turn: CSS Selectors",
    "text": "Your Turn: CSS Selectors\nConstruct a CSS selector that will get all mathematicians from this list without any extra links."
  },
  {
    "objectID": "slides/01-02-html-xml-primer.html#example-mathematicians",
    "href": "slides/01-02-html-xml-primer.html#example-mathematicians",
    "title": "HTML, XML, CSS, and XPath",
    "section": "Example: Mathematicians",
    "text": "Example: Mathematicians\n\nRPython\n\n\n\nlibrary(xml2)\nlibrary(rvest)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(tibble)\n\nurl&lt;-\"https://en.wikipedia.org/wiki/List_of_mathematicians_born_in_the_19th_century\"\nppl &lt;- read_html(url) |&gt;\n    html_nodes(\".mw-body-content ul li\")\nppl[[1]]\n\n{html_node}\n&lt;li&gt;\n[1] &lt;a href=\"/wiki/Florence_Eliza_Allen\" title=\"Florence Eliza Allen\"&gt;Florenc ...\n\n\n\n\n\nfrom bs4 import BeautifulSoup, SoupStrainer\nimport urllib.request\nimport pandas as pd\n\nurl = \"https://en.wikipedia.org/wiki/List_of_mathematicians_born_in_the_19th_century\"\nreq = urllib.request.Request(url)\npage_bytearray = urllib.request.urlopen(req)\npage = page_bytearray.read()\npage_bytearray.close()\n\nsoup = BeautifulSoup(page)\nppl = soup.select(\".mw-body-content ul li\")\nppl[0]\n\n&lt;li&gt;&lt;a href=\"/wiki/Florence_Eliza_Allen\" title=\"Florence Eliza Allen\"&gt;Florence Eliza Allen&lt;/a&gt; (1876–1960)&lt;/li&gt;"
  },
  {
    "objectID": "slides/01-02-html-xml-primer.html#example-mathematicians-1",
    "href": "slides/01-02-html-xml-primer.html#example-mathematicians-1",
    "title": "HTML, XML, CSS, and XPath",
    "section": "Example: Mathematicians",
    "text": "Example: Mathematicians\nNot all HTML nodes have the same attributes/children.\nPreemptive error handling can be helpful.\n\nRPython\n\n\n\ntry_na &lt;- function(i, fn, ...) {\n    res &lt;- try(fn(i, ...))\n    if( \"try-error\" %in% class(res)) {\n        res &lt;- NA\n    }\n    if(length(res) == 0) {\n        res &lt;- NA\n    }\n    res\n}\n\ntry_na() will return\n\nthe value if one exists,\nNA if the command results in an error\nNA if the result has 0 length\n\n\n\n\ndef try_na(x, expression):\n  # If x is NA, then the result must also be NA\n  # for most HTML-parsing expressions... NOT FOOLPROOF\n  if pd.isna(x):\n    return pd.NA\n  else:\n    try:\n      res = eval(expression, {}, {\"x\": x})\n    except:\n      return pd.NA\n    if res is None: # Tests for an empty return value\n      return pd.NA\n    if len(res) == 0:\n      return pd.NA\n  return res"
  },
  {
    "objectID": "slides/01-02-html-xml-primer.html#example-mathematicians-2",
    "href": "slides/01-02-html-xml-primer.html#example-mathematicians-2",
    "title": "HTML, XML, CSS, and XPath",
    "section": "Example: Mathematicians",
    "text": "Example: Mathematicians\n\nRPython\n\n\n\nmath_ppl &lt;- tibble(\n    content = html_text(ppl),\n    link_info = map(ppl, ~try_na(., fn = html_children)),\n    name = map_chr(link_info, ~try_na(., fn = html_text)),\n    name2 = map_chr(link_info, ~try_na(., fn = html_attr, \"title\")),\n    link = map_chr(link_info, ~try_na(., fn = html_attr, \"href\"))\n) |&gt;\n    select(-link_info)\n\nError in xml_text(x, trim = trim) : Unexpected node type\nError in xml_text(x, trim = trim) : Unexpected node type\nError in xml_attr(x, name, default = default) : Unexpected node type\nError in xml_attr(x, name, default = default) : Unexpected node type\nError in xml_attr(x, name, default = default) : Unexpected node type\nError in xml_attr(x, name, default = default) : Unexpected node type\n\nhead(math_ppl)\n\n# A tibble: 6 × 4\n  content                                                      name  name2 link \n  &lt;chr&gt;                                                        &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 Florence Eliza Allen (1876–1960)                             Flor… Flor… /wik…\n2 Emil Artin (1898–1962)                                       Emil… Emil… /wik…\n3 George David Birkhoff (1884–1944)                            Geor… Geor… /wik…\n4 Maxime Bôcher (1867–1918)                                    Maxi… Maxi… /wik…\n5 Leonard Eugene Dickson (1874–1954), algebra and number theo… Leon… Leon… /wik…\n6 Jesse Douglas (1897–1965), Fields Medalist                   Jess… Jess… /wik…\n\n\n\n\n\ncontent = [try_na(i, \"x.text\") for i in ppl]\nlink_info = [try_na(i, \"x.find('a')\") for i in ppl]\nname = [try_na(i, 'x.text') for i in link_info]\nname2 = [try_na(i, 'x.attrs[\"title\"]') for i in link_info]\nlink = [try_na(i, 'x.attrs[\"href\"]') for i in link_info]\nmath_ppl = pd.DataFrame({'content': content, 'name': name, 'name2': name2, 'link': link})\n\nmath_ppl.head()\n\n                                             content  ...                          link\n0                   Florence Eliza Allen (1876–1960)  ...    /wiki/Florence_Eliza_Allen\n1                             Emil Artin (1898–1962)  ...              /wiki/Emil_Artin\n2                  George David Birkhoff (1884–1944)  ...         /wiki/George_Birkhoff\n3                          Maxime Bôcher (1867–1918)  ...      /wiki/Maxime_B%C3%B4cher\n4  Leonard Eugene Dickson (1874–1954), algebra an...  ...  /wiki/Leonard_Eugene_Dickson\n\n[5 rows x 4 columns]"
  },
  {
    "objectID": "assignment-repos/05-pdf-parsing/tilt.html",
    "href": "assignment-repos/05-pdf-parsing/tilt.html",
    "title": "PDF Data TILT Information",
    "section": "",
    "text": "Data scientists and statisticians must be able to take data that is in its natural, messy form and transform it into tidy data, making and documenting the transformations and considering their impact on inference and interpretation of the data. Many organizations release data in PDF format that they do not release in a more standard text-based format (CSV, XLSX, XML, etc.), and sometimes, it is necessary to extract information from these PDF files. This lab is designed to help you practice the skills necessary to extract data from PDFs, so that if you are ever in an unfortunate situation where you need to extract data from a PDF file, you have some familiarity with the tools and the process.\n\n\nThis assignment will help you practice the following skills which are important for being able to access and work with PDF files:\n\nIdentify the type of PDF file\nDevelop a strategy for accessing the data, determining whether it is necessary to use OCR, text-processing, or other tools to get the data out of the PDF file.\nExtract the text data from the PDF file\nClean and format the extracted text to produce tabular data for analysis\n\nData cleaning and wrangling skills\nText processing and regular expression skills\n\nImplement quality control checks to correct common errors which occur during the extraction process\n\n\n\n\nThis assignment will help you to become familiar with important knowledge in this discipline:\n\nPDF file structure (which is important for making accessible documents in addition to OCR and data extraction) and formats\nUse of new libraries to accomplish a task (tesseract, tabulapdf, camelot, etc.)"
  },
  {
    "objectID": "assignment-repos/05-pdf-parsing/tilt.html#purpose",
    "href": "assignment-repos/05-pdf-parsing/tilt.html#purpose",
    "title": "PDF Data TILT Information",
    "section": "",
    "text": "Data scientists and statisticians must be able to take data that is in its natural, messy form and transform it into tidy data, making and documenting the transformations and considering their impact on inference and interpretation of the data. Many organizations release data in PDF format that they do not release in a more standard text-based format (CSV, XLSX, XML, etc.), and sometimes, it is necessary to extract information from these PDF files. This lab is designed to help you practice the skills necessary to extract data from PDFs, so that if you are ever in an unfortunate situation where you need to extract data from a PDF file, you have some familiarity with the tools and the process.\n\n\nThis assignment will help you practice the following skills which are important for being able to access and work with PDF files:\n\nIdentify the type of PDF file\nDevelop a strategy for accessing the data, determining whether it is necessary to use OCR, text-processing, or other tools to get the data out of the PDF file.\nExtract the text data from the PDF file\nClean and format the extracted text to produce tabular data for analysis\n\nData cleaning and wrangling skills\nText processing and regular expression skills\n\nImplement quality control checks to correct common errors which occur during the extraction process\n\n\n\n\nThis assignment will help you to become familiar with important knowledge in this discipline:\n\nPDF file structure (which is important for making accessible documents in addition to OCR and data extraction) and formats\nUse of new libraries to accomplish a task (tesseract, tabulapdf, camelot, etc.)"
  },
  {
    "objectID": "assignment-repos/05-pdf-parsing/tilt.html#success-criteria",
    "href": "assignment-repos/05-pdf-parsing/tilt.html#success-criteria",
    "title": "PDF Data TILT Information",
    "section": "Success Criteria",
    "text": "Success Criteria\n\nGeneral Criteria\n\nStudent’s name is included at top of assignment\nDocument compiles on a different machine\nCompiled document is formatted well\nAll code in the document is contained in appropriately formatted code chunks\nCompiled document does not include long sections of printed data, making use of head and tail commands to show only a few rows of data during data cleaning.\nStudent answers are below the --- separator and above the next prompt, or in cases where questions are enumerated, student answers are indicated using the quote indicator, &gt; and the placeholder text Your answer here has been removed and replaced with the answer.\n\n\n\nTask specific Criteria\n\nWarming Up\n\nPDF Format\n\nPDF file type is identified correctly\nProcessing approach is described correctly and justified based on the PDF format type\n\nPDF Structure\n\nAnswer addresses consistency of data structure in the PDF across years and departments\nlist includes 3-5 reasonable problems which must be overcome to get well-formatted data from the pdf tables\nScreenshots (if included) use appropriate markdown syntax, captions, and hyperlinks\nDiscussion of problems includes a targeted explanation of why PDF data extraction is challenging due to the data format\n\nPlan of Attack\n\nStrategy addresses the problems identified in the previous step and is realistic.\nRelevant tools and functions are included in the plan of attack.\nExplanation of why the strategy minimizes the amount of post-processing (error correction) is reasonable.\n\nAcquiring Metadata\n\nPDF library is used to get metadata information from each PDF\nFunctional approach is used to construct the metadata table\nAt least two anomalies are identified in the metadata\nReasonable explanations for the anomalies are proposed.\nExplanations for unfamiliar components of the metadata are provided.\n\nAnomalies and Strategy Adjustments\n\nRelevant differences in metadata across files are identified and evaluated\nExplanation of evaluation of whether differences are problematic is reasonable\nCode chunk (if provided) explores the anomalies in a sensible way\n\n\nExtracting the Text\n\nIdentify Relevant Pages\n\nFunction is written according to specifications – argument is path, returns pages (either start and end or page range) that have tables\nTable is created with columns as specified\n\nRead in Relevant Text\n\nread_salary_data function created with arguments file, start, end\nWithin the function, only pages between start and end are returned\nProcess is reasonable for 2025-26 feport\nfunction calls pdf_text or read_pdf\n\nPlan Your Approach\n\nList of processing steps is sufficiently detailed to transform text vector into a table with the appropriate columns.\nSteps which are not generalizable are indicated\nInformation which is sacrificed is identified clearly\nAnswer is properly formatted\n\nWould Coordinates Help?\n\nAbstract idea of coordinates for each piece of text is explored\nReasonable compare/contrast to text vector method is provided\n\nExplore Package Documentation\n\nFunction to provide coordinates for each text component is identified in either R or Python\nSteps are clearly identified to make use of coordinate information for table processing\nChallenges for working with coordinate data identified\n\nA Classical Problem\n\nSalary data obtained for all Classics department employees over 4 reports\nSalary data is formatted for analysis\nPlot comparing salaries over years for each individual is present and contains labels and a title\nIndividual salary plot is described with 2-4 sentences\nPlot containing total budget broken out by faculty, administration, and staff is present and contains labels and a title\nOverall budget plot is described with 2-4 sentences\nQuestions provided are addressed in long-form responses with references to the plot. Answers are within the context of the data and the environment in which the data was collected.\n\nQuality Control\n\nAnswer accounts for other facets of the data not present within a single department\nAnswer and reasoning are explained within the context of the problem\nQC measures proposed are reasonable and address problems identified."
  },
  {
    "objectID": "assignment-repos/04-apis/tilt.html",
    "href": "assignment-repos/04-apis/tilt.html",
    "title": "API TILT Information",
    "section": "",
    "text": "Data scientists and statisticians must be able to take data that is in its natural, messy form and transform it into tidy data, making and documenting the transformations and considering their impact on inference and interpretation of the data. One efficient way to access online data is through application programming interfaces (APIs). In this assignment, you will practice using several different APIs to accomplish different tasks which might come up during work as a data scientist or statistician.\n\n\nThis assignment will help you practice the following skills which are important for being able to access and work with APIs:\n\nUse an API via a URL\nMake API calls after setting header information\nMake API calls using an API key\nUse best practices to secure API key information\n\n\n\n\nThis assignment will help you to become familiar with important knowledge in this discipline:\n\nHow to read API documentation and determine how to extract the required information from the API\nHTTP response codes and how to interpret them successfully\nEthics and manners for web scraping and API use"
  },
  {
    "objectID": "assignment-repos/04-apis/tilt.html#purpose",
    "href": "assignment-repos/04-apis/tilt.html#purpose",
    "title": "API TILT Information",
    "section": "",
    "text": "Data scientists and statisticians must be able to take data that is in its natural, messy form and transform it into tidy data, making and documenting the transformations and considering their impact on inference and interpretation of the data. One efficient way to access online data is through application programming interfaces (APIs). In this assignment, you will practice using several different APIs to accomplish different tasks which might come up during work as a data scientist or statistician.\n\n\nThis assignment will help you practice the following skills which are important for being able to access and work with APIs:\n\nUse an API via a URL\nMake API calls after setting header information\nMake API calls using an API key\nUse best practices to secure API key information\n\n\n\n\nThis assignment will help you to become familiar with important knowledge in this discipline:\n\nHow to read API documentation and determine how to extract the required information from the API\nHTTP response codes and how to interpret them successfully\nEthics and manners for web scraping and API use"
  },
  {
    "objectID": "assignment-repos/04-apis/tilt.html#success-criteria",
    "href": "assignment-repos/04-apis/tilt.html#success-criteria",
    "title": "API TILT Information",
    "section": "Success Criteria",
    "text": "Success Criteria\n\nGeneral Criteria\n\nStudent’s name is included at top of assignment\nDocument compiles on a different machine\nCompiled document is formatted well\nAll code in the document is contained in appropriately formatted code chunks\nCompiled document does not include long sections of printed data, making use of head and tail commands to show only a few rows of data during data cleaning.\nStudent answers are below the --- separator and above the next prompt, or in cases where questions are enumerated, student answers are indicated using the quote indicator, &gt; and the placeholder text Your answer here has been removed and replaced with the answer.\n\n\n\nTask specific Criteria\n\nOverall criteria\n\nAPI key is protected and not pushed to GitHub or otherwise exposed.\n\n\n\nPublic API, No Authentication\nHTTP Response Codes - [ ] Table of HTTP response codes scraped - [ ] Table is named correctly - [ ] Table has columns that are named as specified\nFighting Like Cats and Dogs with APIs 1. Saving the response images\n\nURLs for images created correctly and saved in url column\nFolder to save images into created as directed\nFunctional programming used to save images to folder\n\nIntroducing glue and f-strings\n\nFormatted string created using glue (R) or f-strings (python) to generate a complete image reference string\nColumns which will be used to fill in the string are indicated\n\nCreating An Image Wall\n\nMarkdown code for all images created successfully\nYAML header created successfully\nStart string for figure panel created\nEnd string for figure panel created\nLines assembled in the correct order with appropriate blank lines\nmarkdown file written to http-codes.qmd\nmarkdown file compiles\nfigure panel works as specified\n\nAPIs with Authentication\nRegister for Calendarific - [ ] API key stored in .Renviron or .env (but not pushed to the repository) - [ ] .gitignore file contains appropriate exclusion - [ ] Code to load the key into the environment is provided (python – R will load it automatically) - [ ] Screenshot of working directory provided in markdown document (and image added to repo and linked correctly) - [ ] Code included to determine how many characters are present in API key - [ ] Code chunk is set to not evaluate - [ ] Number of characters in key is correct and included statically\nOctober Holidays - [ ] Query constructed to get all US holidays in October - [ ] Code to run query is correct and works with an API key set up as directed - [ ] Query response saved to oct_holidays and written out to a (binary) file (so as not to expose the API key) - [ ] Thoughtful and correct answer to minimize redundant API calls question\nFormatting JSON data - [ ] Response body saved as oct-holidays.json - [ ] Holidays formatted in a rectangular table with only states as a nested column; columns are otherwise as described. - [ ] Dates are stored as dates, not characters\nState Holidays - [ ] State specific holidays separated from national holidays - [ ] State specific holidays unnested successfully - [ ] Unnested data are combined back together appropriately to get a dataset which has one row for each entity celebrating the holiday\nHoliday List - [ ] Holidays are appropriately merged, keeping only the national holiday in cases where there are the same national and state holiday on the same date. - [ ] Table is sorted by date - [ ] Table includes only holidays that are US federal holidays, Nebraska holidays, religious observances, or world statistics day. - [ ] Code is used to present the results as a markdown formatted table integrated into the document.\nDate Sequences - [ ] long_holiday(data, \"name\") function created as specified to expand holidays that have First Day of X, Last Day of X labels. - [ ] Function is tested and works correctly\nScheduling an Event - [ ] Date range only includes August - December - [ ] Saturdays and Sundays are excluded - [ ] holidays are excluded - [ ] Plot shows a reasonable representation of a calendar"
  },
  {
    "objectID": "assignment-repos/02-list-processing/tilt.html",
    "href": "assignment-repos/02-list-processing/tilt.html",
    "title": "List Processing TILT Information",
    "section": "",
    "text": "Data scientists and statisticians must be able to take data that is in its natural, messy form and transform it into tidy data, making and documenting the transformations and considering their impact on inference and interpretation of the data. Many data formats, such as JSON, XML, and YAML, do not assume a rectangular data format where observations are in rows and variables are in columns. There are also many situations where data may be nested or networked in its natural form. It is important to be able to convert between formats, identifying the critical pieces of information in any data structure and developing a strategy to convert the data into something that can be analyzed or visualized effectively. This assignment will help you understand and leverage list processing techniques to tidy nested and record-based data.\n\n\nThis assignment will help you practice the following skills which are important for being able to access and work with data:\n\nIdentify the structure of JSON and XML files\nUse functional programming to apply functions to lists in order to extract and/or process data efficiently\nTransform record-based formats into one or more rectangular table(s) that may be linked by a key variable.\nIdentify areas where quality control checks may be necessary when converting data between record-based and tabular formats\n\n\n\n\nThis assignment will help you to become familiar with important knowledge in this discipline:\n\nRecord-based data structures\nReading and constructing database schema (descriptions of variables in data tables)\nFunctional programming techniques"
  },
  {
    "objectID": "assignment-repos/02-list-processing/tilt.html#purpose",
    "href": "assignment-repos/02-list-processing/tilt.html#purpose",
    "title": "List Processing TILT Information",
    "section": "",
    "text": "Data scientists and statisticians must be able to take data that is in its natural, messy form and transform it into tidy data, making and documenting the transformations and considering their impact on inference and interpretation of the data. Many data formats, such as JSON, XML, and YAML, do not assume a rectangular data format where observations are in rows and variables are in columns. There are also many situations where data may be nested or networked in its natural form. It is important to be able to convert between formats, identifying the critical pieces of information in any data structure and developing a strategy to convert the data into something that can be analyzed or visualized effectively. This assignment will help you understand and leverage list processing techniques to tidy nested and record-based data.\n\n\nThis assignment will help you practice the following skills which are important for being able to access and work with data:\n\nIdentify the structure of JSON and XML files\nUse functional programming to apply functions to lists in order to extract and/or process data efficiently\nTransform record-based formats into one or more rectangular table(s) that may be linked by a key variable.\nIdentify areas where quality control checks may be necessary when converting data between record-based and tabular formats\n\n\n\n\nThis assignment will help you to become familiar with important knowledge in this discipline:\n\nRecord-based data structures\nReading and constructing database schema (descriptions of variables in data tables)\nFunctional programming techniques"
  },
  {
    "objectID": "assignment-repos/02-list-processing/tilt.html#success-criteria",
    "href": "assignment-repos/02-list-processing/tilt.html#success-criteria",
    "title": "List Processing TILT Information",
    "section": "Success Criteria",
    "text": "Success Criteria\n\nGeneral Criteria\n\nStudent’s name is included at top of assignment\nDocument compiles on a different machine\nCompiled document is formatted well\nAll code in the document is contained in appropriately formatted code chunks\nCompiled document does not include long sections of printed data, making use of head and tail commands to show only a few rows of data during data cleaning.\nStudent answers are below the --- separator and above the next prompt, or in cases where questions are enumerated, student answers are indicated using the quote indicator, &gt; and the placeholder text Your answer here has been removed and replaced with the answer.\n\n\n\nTask specific Criteria\n\nWarming Up\n\nParse the file\n\nCode chunk is added correctly\nThree drwhoYYYY objects are created: drwho1963, drwho2005, and drwho2023\n[ ]\n\nExamining List Data Structures\n\nList structure is properly indented to show different levels of nesting\nList structure is in valid markdown and renders as an unordered, nested list\nRedundant data question is answered thoughtfully\n\nDevelop a Strategy\n\nRectangular data structure contains all relevant information\nSketch is included successfully, either using one or more markdown tables or an image.\nA reasonable sequence of operations are provided in an ordered list\n\nImplement Your Strategy\n\nCode chunk is added correctly\nJSON files are converted into tabular data\nVariables in tabular data are correctly formatted using appropriate types\nFirst 5 rows of each table are printed out\n\nExamining Episode Air Dates\n\nPlot has appropriate axes, axis labels, and title\nThe geom used in the plot is appropriate for displaying the data\nPlot shows length between adjacent episodes by season across all seasons\nDifferent series of Dr. Who are handled successfully\nPlot is described appropriately and any visually interesting patterns are mentioned.\n\n\nTimey-Wimey Series and Episodes\n\nSetting Up\n\nCode to read in files looks for JSON files in the working directory and then applies a function to read in data from each file (e.g. functional programming)\nProcessing code is very similar to that used in previous sections – that is, the code is flexible enough to be reused.\ndata table is appropriately named\ndata table generated is reasonably tidy and has appropriate types in the columns\n\nAir Time\n\nInvestigative code is provided and follows a logical sequence\nConclusions are drawn relating airtimes to target audiences.\nShows without airtimes (Torchwood: Web of Lies) are explained in a reasonable manner.\n\nAnother Layer of JSON\n\nJSON files are read in using a functional approach\nMinimal server calls are used\nKeys provided for joins are reasonable, and meet the technical definition of a key.\n\nExplore!\n\nQuestion requires at least a couple of processing steps before it is possible to answer.\nGraphics or tables are well formatted and selected to convey appropriate information.\nGraphics or tables are explained with at least 2-3 sentences\nCode makes use of functional programming."
  },
  {
    "objectID": "assignment-repos/01-web-scraping/tilt.html",
    "href": "assignment-repos/01-web-scraping/tilt.html",
    "title": "Web Scraping TILT Information",
    "section": "",
    "text": "In this assignment, you will decode file formats that are XML-based, pulling out relevant information programmatically and implementing quality-control measures for that data. The techniques you practice here may be applied to pull data out of documents with common formats, map data, and other information provided online, as XML and HTML documents are involved in everything online.\n\n\nThis assignment will help you practice the following skills which are important for being able to access and work with data:\n\nReading and using XML data schemas\nDeveloping strategies to extract data from an XML document using XPATH and CSS selectors\nExtracting data from XML-coded documents\nReformatting and tidying extracted data into a form that is workable for statistical analysis\nImplementing quality control measures for data assembled from sources which are not always completely consistent\n\n\n\n\nThis assignment will help you to become familiar with important knowledge in this discipline:\n\nXML data formats\nTidy data formats\nFunctional programming techniques\nComputational time considerations for different strategies for data extraction\nFile access time considerations for different data extraction strategies"
  },
  {
    "objectID": "assignment-repos/01-web-scraping/tilt.html#purpose",
    "href": "assignment-repos/01-web-scraping/tilt.html#purpose",
    "title": "Web Scraping TILT Information",
    "section": "",
    "text": "In this assignment, you will decode file formats that are XML-based, pulling out relevant information programmatically and implementing quality-control measures for that data. The techniques you practice here may be applied to pull data out of documents with common formats, map data, and other information provided online, as XML and HTML documents are involved in everything online.\n\n\nThis assignment will help you practice the following skills which are important for being able to access and work with data:\n\nReading and using XML data schemas\nDeveloping strategies to extract data from an XML document using XPATH and CSS selectors\nExtracting data from XML-coded documents\nReformatting and tidying extracted data into a form that is workable for statistical analysis\nImplementing quality control measures for data assembled from sources which are not always completely consistent\n\n\n\n\nThis assignment will help you to become familiar with important knowledge in this discipline:\n\nXML data formats\nTidy data formats\nFunctional programming techniques\nComputational time considerations for different strategies for data extraction\nFile access time considerations for different data extraction strategies"
  },
  {
    "objectID": "assignment-repos/01-web-scraping/tilt.html#success-criteria",
    "href": "assignment-repos/01-web-scraping/tilt.html#success-criteria",
    "title": "Web Scraping TILT Information",
    "section": "Success Criteria",
    "text": "Success Criteria\nNote: These criteria are provided to facilitate self reflection and evaluation. Assignment grades may be decided based on an instructor-selected subset of these criteria.\n\nGeneral Criteria\n\nStudent’s name is included at top of assignment\nDocument compiles on a different machine\nCompiled document is formatted well\nAll code in the document is contained in appropriately formatted code chunks\nCompiled document does not include long sections of printed data, making use of head and tail commands to show only a few rows of data during data cleaning.\nStudent answers are below the --- separator and above the next prompt, or in cases where questions are enumerated, student answers are indicated using the quote indicator, &gt; and the placeholder text Your answer here has been removed and replaced with the answer.\n\n\n\nTask-specific criteria\n\nWarming Up\n\nParsing\n\nCode chunk reads in the RSS feed from the web address\nRSS feed is saved as an xml-parseable object\n\nExtracting Information\n\nResulting data frame has columns title, link, summary, updated, category, and id\nCode demonstrates accessing node attribute(s)\nCode demonstrates accessing node content\n\nIs it XML?\n\nResponse lists requirements for valid XML files\nResponse evaluates each listed requirement of an XML file accurately\n\n\nCleaning the Data\n\nTidy Data\n\nResponse identifies aspects of the data which are not tidy\nEach aspect of the data which is identified as messy is related back to a violation of tidy criteria (one value in a cell, one variable in each column, one observation in each row).\n\nTidy Planning\n\nSteps to transition from current form to tidy form are listed in order\nListed steps are adequate to complete the transition\nEach step contains the necessary information to identify a function and provide the required parameters for that function. For instance, if the step is “Separate values in column 3 into two separate columns”, the description should also include the delimiter or regular expression used to separate the values, and the names of the columns the data will be extracted into.\n\nTidying, for real!\n\nCode included in this step matches the planning in step 2\nCode works correctly\nResulting data shown contains less than 10 entries\nData shown is formatted using a\n\n\nFiling Forms\n\nPreparing\n\nCSS selector for the table is correctly identified\nCSS selector for the first link in the table is correctly identified\n\nReading HTML Tables\n\nFunction extracts the .htm link from the table when provided with the filing detail URL\nHTML addresses for all filed forms are obtained successfully\n\nReading HTML Pages\n\nRequisite data is extracted and stored in a data frame\n(Suggested) Functions are used to modularize data extraction\nData extraction code effectively handles missing data without erroring out\n\nAssessing Market Value of Stock Sales\n\nAggregate market value of stock is converted to a numerical variable\nAgg. market value of stock is used to make a histogram\nHistogram y-axis indicates that values are counts\nHistogram x-axis indicates that the plotted value is the aggregate market value of the stock\nHistogram has an accurate title\nIf histogram values cover several orders of magnitude, an appropriate scale transformation is used\n\nEfficiency\n\nOrdered list of steps is complete\nSteps minimize the number of calls to the SEC server\nSteps de-duplicate any repeated entries\nTotal number of calls to the SEC server is calculated correctly\nCalculation of total number of calls is explained"
  },
  {
    "objectID": "weeks/week-17.html",
    "href": "weeks/week-17.html",
    "title": "Week 17: Finals",
    "section": "",
    "text": "December 14-20, 2025\n\n\n\nFinal Exam Due"
  },
  {
    "objectID": "weeks/week-17.html#practice-your-skills",
    "href": "weeks/week-17.html#practice-your-skills",
    "title": "Week 17: Finals",
    "section": "",
    "text": "Final Exam Due"
  },
  {
    "objectID": "weeks/week-15.html",
    "href": "weeks/week-15.html",
    "title": "Week 15: Special Topics",
    "section": "",
    "text": "Week 15: Special Topics\nNovember 30-06, 2025",
    "crumbs": [
      "Weekly materials",
      "Week 15 - Special Topics"
    ]
  },
  {
    "objectID": "weeks/week-13.html",
    "href": "weeks/week-13.html",
    "title": "Week 13: DuckDB",
    "section": "",
    "text": "Week 13: DuckDB\nNovember 16-22, 2025",
    "crumbs": [
      "Weekly materials",
      "Week 13 - DuckDB"
    ]
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 11: SQL and SQLite",
    "section": "",
    "text": "Week 11: SQL and SQLite\nNovember 02-08, 2025",
    "crumbs": [
      "Weekly materials",
      "Week 11 - SQL/SQLite"
    ]
  },
  {
    "objectID": "weeks/week-09.html",
    "href": "weeks/week-09.html",
    "title": "Week 9: Linked Interactive Graphics",
    "section": "",
    "text": "October 19-25, 2025\n\n\n\nCubble vignette\nmpld3 in python?\n\n\n\n\n\nFall Break!\n\n\n\n\n\nLecture: Interactive Linked Graphics\n\n\n\n\n\nScreencast due!",
    "crumbs": [
      "Weekly materials",
      "Week 9 - Linked Graphics"
    ]
  },
  {
    "objectID": "weeks/week-09.html#reading",
    "href": "weeks/week-09.html#reading",
    "title": "Week 9: Linked Interactive Graphics",
    "section": "",
    "text": "Cubble vignette\nmpld3 in python?",
    "crumbs": [
      "Weekly materials",
      "Week 9 - Linked Graphics"
    ]
  },
  {
    "objectID": "weeks/week-09.html#tuesday",
    "href": "weeks/week-09.html#tuesday",
    "title": "Week 9: Linked Interactive Graphics",
    "section": "",
    "text": "Fall Break!",
    "crumbs": [
      "Weekly materials",
      "Week 9 - Linked Graphics"
    ]
  },
  {
    "objectID": "weeks/week-09.html#thursday",
    "href": "weeks/week-09.html#thursday",
    "title": "Week 9: Linked Interactive Graphics",
    "section": "",
    "text": "Lecture: Interactive Linked Graphics",
    "crumbs": [
      "Weekly materials",
      "Week 9 - Linked Graphics"
    ]
  },
  {
    "objectID": "weeks/week-09.html#practice-your-skills",
    "href": "weeks/week-09.html#practice-your-skills",
    "title": "Week 9: Linked Interactive Graphics",
    "section": "",
    "text": "Screencast due!",
    "crumbs": [
      "Weekly materials",
      "Week 9 - Linked Graphics"
    ]
  },
  {
    "objectID": "weeks/week-07.html",
    "href": "weeks/week-07.html",
    "title": "Week 7: Dynamic Communication with Shiny",
    "section": "",
    "text": "October 05-11, 2025\n\n\n\nInteractive Graphics (Shiny)\n\n\n\n\n\nLook over Shiny documentation to go through the basic tutorial in R and/or python.\n\n\n\n\n\nLecture: Shiny\n\n\n\n\n\nLab: Shiny Applets\n\n\n\n\n\nLab: Shiny Applets\nLab goals",
    "crumbs": [
      "Weekly materials",
      "Week 7 - Shiny"
    ]
  },
  {
    "objectID": "weeks/week-07.html#reading",
    "href": "weeks/week-07.html#reading",
    "title": "Week 7: Dynamic Communication with Shiny",
    "section": "",
    "text": "Interactive Graphics (Shiny)",
    "crumbs": [
      "Weekly materials",
      "Week 7 - Shiny"
    ]
  },
  {
    "objectID": "weeks/week-07.html#prepare-for-class",
    "href": "weeks/week-07.html#prepare-for-class",
    "title": "Week 7: Dynamic Communication with Shiny",
    "section": "",
    "text": "Look over Shiny documentation to go through the basic tutorial in R and/or python.",
    "crumbs": [
      "Weekly materials",
      "Week 7 - Shiny"
    ]
  },
  {
    "objectID": "weeks/week-07.html#tuesday",
    "href": "weeks/week-07.html#tuesday",
    "title": "Week 7: Dynamic Communication with Shiny",
    "section": "",
    "text": "Lecture: Shiny",
    "crumbs": [
      "Weekly materials",
      "Week 7 - Shiny"
    ]
  },
  {
    "objectID": "weeks/week-07.html#thursday",
    "href": "weeks/week-07.html#thursday",
    "title": "Week 7: Dynamic Communication with Shiny",
    "section": "",
    "text": "Lab: Shiny Applets",
    "crumbs": [
      "Weekly materials",
      "Week 7 - Shiny"
    ]
  },
  {
    "objectID": "weeks/week-07.html#practice-your-skills",
    "href": "weeks/week-07.html#practice-your-skills",
    "title": "Week 7: Dynamic Communication with Shiny",
    "section": "",
    "text": "Lab: Shiny Applets\nLab goals",
    "crumbs": [
      "Weekly materials",
      "Week 7 - Shiny"
    ]
  },
  {
    "objectID": "weeks/week-05.html",
    "href": "weeks/week-05.html",
    "title": "Week 5: PDF Tools",
    "section": "",
    "text": "September 21-27, 2025\n\n\n\nWorking with PDFs\n\n\n\n\n\nFind examples of image-based and text-based PDFs\n\n\n\n\n\nLecture: PDF Tools\n\n\n\n\n\nLab: PDF/OCR\n\n\n\n\n\nLab: PDF/OCR\nLab goals",
    "crumbs": [
      "Weekly materials",
      "Week 5 - PDF Tools"
    ]
  },
  {
    "objectID": "weeks/week-05.html#reading",
    "href": "weeks/week-05.html#reading",
    "title": "Week 5: PDF Tools",
    "section": "",
    "text": "Working with PDFs",
    "crumbs": [
      "Weekly materials",
      "Week 5 - PDF Tools"
    ]
  },
  {
    "objectID": "weeks/week-05.html#prepare-for-class",
    "href": "weeks/week-05.html#prepare-for-class",
    "title": "Week 5: PDF Tools",
    "section": "",
    "text": "Find examples of image-based and text-based PDFs",
    "crumbs": [
      "Weekly materials",
      "Week 5 - PDF Tools"
    ]
  },
  {
    "objectID": "weeks/week-05.html#tuesday",
    "href": "weeks/week-05.html#tuesday",
    "title": "Week 5: PDF Tools",
    "section": "",
    "text": "Lecture: PDF Tools",
    "crumbs": [
      "Weekly materials",
      "Week 5 - PDF Tools"
    ]
  },
  {
    "objectID": "weeks/week-05.html#thursday",
    "href": "weeks/week-05.html#thursday",
    "title": "Week 5: PDF Tools",
    "section": "",
    "text": "Lab: PDF/OCR",
    "crumbs": [
      "Weekly materials",
      "Week 5 - PDF Tools"
    ]
  },
  {
    "objectID": "weeks/week-05.html#practice-your-skills",
    "href": "weeks/week-05.html#practice-your-skills",
    "title": "Week 5: PDF Tools",
    "section": "",
    "text": "Lab: PDF/OCR\nLab goals",
    "crumbs": [
      "Weekly materials",
      "Week 5 - PDF Tools"
    ]
  },
  {
    "objectID": "weeks/week-03.html",
    "href": "weeks/week-03.html",
    "title": "Week 3: Record-based Data and List Processing",
    "section": "",
    "text": "September 07-13, 2025\n\n\n\nFunctional Programming\nRecord-based Data and List Processing\n\n\n\n\n\nLecture: Record based Data and List Processing\n\n\n\n\n\nLab: Parsing XML Files\n\n\n\n\n\nLab: Parsing XML Files\nLab goals",
    "crumbs": [
      "Weekly materials",
      "Week 3 - Records & Lists"
    ]
  },
  {
    "objectID": "weeks/week-03.html#reading",
    "href": "weeks/week-03.html#reading",
    "title": "Week 3: Record-based Data and List Processing",
    "section": "",
    "text": "Functional Programming\nRecord-based Data and List Processing",
    "crumbs": [
      "Weekly materials",
      "Week 3 - Records & Lists"
    ]
  },
  {
    "objectID": "weeks/week-03.html#tuesday",
    "href": "weeks/week-03.html#tuesday",
    "title": "Week 3: Record-based Data and List Processing",
    "section": "",
    "text": "Lecture: Record based Data and List Processing",
    "crumbs": [
      "Weekly materials",
      "Week 3 - Records & Lists"
    ]
  },
  {
    "objectID": "weeks/week-03.html#thursday",
    "href": "weeks/week-03.html#thursday",
    "title": "Week 3: Record-based Data and List Processing",
    "section": "",
    "text": "Lab: Parsing XML Files",
    "crumbs": [
      "Weekly materials",
      "Week 3 - Records & Lists"
    ]
  },
  {
    "objectID": "weeks/week-03.html#practice-your-skills",
    "href": "weeks/week-03.html#practice-your-skills",
    "title": "Week 3: Record-based Data and List Processing",
    "section": "",
    "text": "Lab: Parsing XML Files\nLab goals",
    "crumbs": [
      "Weekly materials",
      "Week 3 - Records & Lists"
    ]
  },
  {
    "objectID": "weeks/week-01.html",
    "href": "weeks/week-01.html",
    "title": "Week 1: Intro & HTML Primer",
    "section": "",
    "text": "August 24-30, 2025\n\n\n\nSyllabus\nWeb Scraping\n\n\n\n\n\nEnsure R, python, quarto, RStudio are installed on your machine (Computer setup help)\nPrint out and bring cheat sheets for R and python (if you need them)\n\n\n\n\n\nDiscussion: Syllabus\n\n\n\n\n\nLecture: HTML/XML primer\n\n\n\n\n\nGet familiar with HTML and web-based vocabulary:\n\nWatch HTTP and HTML explanation\nWatch Web Scraping with BeautifulSoup and Requests\nWatch Control a web browser from R to web scrape static and dynamic websites using {chromote}",
    "crumbs": [
      "Weekly materials",
      "Week 1 - Intro & HTML Primer"
    ]
  },
  {
    "objectID": "weeks/week-01.html#reading",
    "href": "weeks/week-01.html#reading",
    "title": "Week 1: Intro & HTML Primer",
    "section": "",
    "text": "Syllabus\nWeb Scraping",
    "crumbs": [
      "Weekly materials",
      "Week 1 - Intro & HTML Primer"
    ]
  },
  {
    "objectID": "weeks/week-01.html#prepare-for-class",
    "href": "weeks/week-01.html#prepare-for-class",
    "title": "Week 1: Intro & HTML Primer",
    "section": "",
    "text": "Ensure R, python, quarto, RStudio are installed on your machine (Computer setup help)\nPrint out and bring cheat sheets for R and python (if you need them)",
    "crumbs": [
      "Weekly materials",
      "Week 1 - Intro & HTML Primer"
    ]
  },
  {
    "objectID": "weeks/week-01.html#tuesday",
    "href": "weeks/week-01.html#tuesday",
    "title": "Week 1: Intro & HTML Primer",
    "section": "",
    "text": "Discussion: Syllabus",
    "crumbs": [
      "Weekly materials",
      "Week 1 - Intro & HTML Primer"
    ]
  },
  {
    "objectID": "weeks/week-01.html#thursday",
    "href": "weeks/week-01.html#thursday",
    "title": "Week 1: Intro & HTML Primer",
    "section": "",
    "text": "Lecture: HTML/XML primer",
    "crumbs": [
      "Weekly materials",
      "Week 1 - Intro & HTML Primer"
    ]
  },
  {
    "objectID": "weeks/week-01.html#practice-your-skills",
    "href": "weeks/week-01.html#practice-your-skills",
    "title": "Week 1: Intro & HTML Primer",
    "section": "",
    "text": "Get familiar with HTML and web-based vocabulary:\n\nWatch HTTP and HTML explanation\nWatch Web Scraping with BeautifulSoup and Requests\nWatch Control a web browser from R to web scrape static and dynamic websites using {chromote}",
    "crumbs": [
      "Weekly materials",
      "Week 1 - Intro & HTML Primer"
    ]
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Stat 351. Statistical Computing II - Data Management and Visualization",
    "section": "",
    "text": "Course Description\nComputational skills for management, visualization and analysis of large and complex data which are necessary for modern statistics. Includes a wide range of topics necessary for data analytics, including harvesting data from websites and common data structures, setting up and working with databases, and designing interactive data displays.\n\n\nLearning Objectives\n\nAccess and leverage data stored in formats which are commonly used outside of statistics (HTML, JSON, XML, PDF, APIs) and transform these data to formats which are used for statistical analysis.\n\nScrape data off of the internet and assemble it into a “tidy” format for visualization and analysis.\nRead in structured data from record-based formats (XML, JSON) and transform this data to a table-based format.\nUse optical character recognition and other tools to extract data from a PDF file systematically.\nUse an API to request data from an online service.\nImplement data cleaning and quality control measures to ensure that data is read in correctly.\n\nDevelop skills for visualization and communication of complex data using interactive graphics. You will be able to\n\nDetermine when an interactive chart is preferable to a static chart.\nCreate an interactive chart using JavaScript-based tools such as Plotly, Observable.js, or Shiny.\nIntegrate your interactive chart into a report or web page, along with supportive text describing the chart and important findings.\n\nUnderstand and leverage data management tools for storing and manipulating data, including\n\nIdentifying situations where an external database is preferable to working with data in-memory.\nAccessing data in an external SQL, Parquet, or Arrow database.\nDiscussing the tradeoffs between different tools for data management and different approaches to data storage.\nDesign an analysis strategy for large data which does not fit into computer memory by selecting from strategies such as sampling and split-apply-combine.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus - Stat 351",
    "section": "",
    "text": "InstructorSusan Vanderplas\n\nE-mailsusan.vanderplas@unl.edu \nOfficeHardin 343D\n\n \nTimeTR 11:00-12:15\nLocationKEIM 214\nStudent HoursSchedule here",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#objectives",
    "href": "syllabus.html#objectives",
    "title": "Syllabus - Stat 351",
    "section": "1.1 Objectives",
    "text": "1.1 Objectives\n\nAccess and leverage data stored in formats which are commonly used outside of statistics (HTML, JSON, XML, PDF, APIs) and transform these data to formats which are used for statistical analysis.\n\nScrape data off of the internet and assemble it into a “tidy” format for visualization and analysis.\nRead in structured data from record-based formats (XML, JSON) and transform this data to a table-based format.\nUse optical character recognition and other tools to extract data from a PDF file systematically.\nUse an API to request data from an online service.\nImplement data cleaning and quality control measures to ensure that data is read in correctly.\n\nDevelop skills for visualization and communication of complex data using interactive graphics. You will be able to\n\nDetermine when an interactive chart is preferable to a static chart.\nCreate an interactive chart using JavaScript-based tools such as Plotly, Observable.js, or Shiny.\nIntegrate your interactive chart into a report or web page, along with supportive text describing the chart and important findings.\n\nUnderstand and leverage data management tools for storing and manipulating data, including\n\nIdentifying situations where an external database is preferable to working with data in-memory.\nAccessing data in an external SQL, Parquet, or Arrow database.\nDiscussing the tradeoffs between different tools for data management and different approaches to data storage.\nDesign an analysis strategy for large data which does not fit into computer memory by selecting from strategies such as sampling and split-apply-combine.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assessmentgrading",
    "href": "syllabus.html#assessmentgrading",
    "title": "Syllabus - Stat 351",
    "section": "3.1 Assessment/Grading 💯 🗒️",
    "text": "3.1 Assessment/Grading 💯 🗒️\n\n\n\nAssignments\nWeight\n\n\n\n\nReading Quizzes & Participation\n10%\n\n\nHomework/Labs\n50%\n\n\nScreencast\n20%\n\n\nFinal Exam\n20%\n\n\n\nLower bounds for grade cutoffs are shown in the following table.\n\n\n\nLetter grade\nX +\nX\nX -\n\n\n\n\nA\n96.5\n93.5\n89.5\n\n\nB\n86.5\n83.5\n79.5\n\n\nC\n76.5\n73.5\n69.5\n\n\nD\n66.5\n63.5\n60.5\n\n\nF\n&lt;60.5\n\n\n\n\n\nInterpretation: A grade of 84.3 will receive a B. A grade of 79.49 will receive a C+. A grade of 73.49 will receive a C-. Anything below a 60.5 will receive an F.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-components",
    "href": "syllabus.html#course-components",
    "title": "Syllabus - Stat 351",
    "section": "3.2 Course Components",
    "text": "3.2 Course Components\n\n3.2.1 Reading Quizzes 📖\nYou will have weekly reading assignments which introduce the weekly focus area and provide different perspectives on the material. Some of these perspectives will be covered in class lectures and activities, but some will not and are provided to expose you to different approaches and opinions.\nIn order to motivate the importance of doing the weekly reading before class, you will be given a weekly reading quiz in class which covers the requisite material.\n\n\n3.2.2 Homework/Lab Assignments 💻\nWeekly formative assignments (assignments meant to help you practice skills) will be given throughout the semester.\n\nYou will typically have one week to work on each of the assignments.\nAssignments must be submitted in the file format specified, and should run or compile as submitted for credit to be given.\nI will attempt to grade formative assignments within a week of the specified due date.\nThe assignment due date in Canvas will be set to at least 48 hours before the hard deadline for assignment submission. This provides an implicit “grace period” for assignment submission. No additional extensions will be provided.\n\n\n\n3.2.3 Screencast\nYou will produce a screencast which demonstrates your skills in data acquisition and data visualization. This project will be submitted around midterms.\n\n\n3.2.4 Final Exam\nA final exam (format TBD) will be used to assess your ability to meet the learning objectives for the course. The exam will be due during the scheduled final exam period for the course (Monday, December 15). It is possible that a take-home portion of the exam may be initially assigned during the last week of classes.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus - Stat 351",
    "section": "3.3 Course Policies",
    "text": "3.3 Course Policies\n\n3.3.1 Late Work Policy ⏰\nLate assignments will be accepted only under extenuating circumstances, and only if you have contacted me prior to the assignment due date and received permission to hand the assignment in late. I reserve the right not to grade (or to assign a 0 to) any assignments received after the assignment due date without prior approval. Most lab assignments build in a “grace period” - they are typically due on Friday and the assignment closes Sunday evening. This additional time serves as a general deadline extension to provide an extra few days to work on the assignment.\n\n\n3.3.2 Attendance & Participation :person:\nYou are expected to attend class and participate in online discussions. Consistent, repeated failure to attend class or actively participate in class will affect the participation portion of your grade.\n\nExcused Absences\nThe university lists several reasons that a student may miss a class or significant component of the course such as an exam Faculty Senate Class Attendance Policy.\n\nillness of self or dependent,\nparticipation in UNL-sponsored activities,\nmilitary service or jury duty,\nbereavement, or\nreligious observance\n\nIn all cases, students are expected to provide notice to the instructor or the university as soon as possible.\nReligious observance requests must be made by the 2nd week of class.\nConflicts with scheduled UNL-sponsored activities (including participation in conferences) should be communicated to the instructor by the 2nd week of class (if scheduled at the start of the semester) or with at least 2 weeks notice. Emergent conflicts will be handled on a case-by-case basis, but prior notice is expected.\nTo initiate bereavement leave, students should contact the Vice Chancellor for Student Affairs, and that office will coordinate with instructors.\nIn the case of extended or chronic illness, the student should work with Disability Services to obtain formal accommodations. Disability Services may require medical documentation as part of the accommodations process.\n\n\n\n\n\n\nNote that scheduling of vacations or international travel is NOT considered a university approved absence.\n\n\n\n\n\nIllness 🤒 & Excused Absences\nIf you are feeling ill, please do not come to class. Instead, review the material and work on the homework/lab assignment, and then schedule an appointment with me to meet virtually within a week of your absence. In the appointment reason field on Calendly, indicate that this appointment is to substitute for your in-class participation on the date you missed.\nIf you need to miss more than 1-2 classes for illness, I reserve the right to require documentation from Disability Services or a medical provider.\n\n\nInclement Weather 🌨️ 🌪️ ⛈️\nIf in-person classes are canceled, you will be notified of the instructional continuity plan for this class by Canvas Announcement. When there is power in the Lincoln area, we may hold class via Zoom; however, I may assign an asynchronous activity instead. You will be held to the same participation and professionalism standards on Zoom as you are in person, and participation and attendance will be graded accordingly. That is – cameras should be on, you should be fully clothed, and you should be an active participant and not an empty black box. If there is a reason you don’t want your camera to be on (e.g. if you’re in an environment you can’t fully control, or you’re sick and don’t want to be on camera), let me know. Pets are 100% welcome to join the zoom call ❤️ 🐶 🐱.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#expectations",
    "href": "syllabus.html#expectations",
    "title": "Syllabus - Stat 351",
    "section": "3.4 Expectations",
    "text": "3.4 Expectations\nYou can expect me to:\n\nreply to emails within 48 hours during the week (72 hours on weekends)\nbe available in class to assist with assignments\nbe available by appointment for additional help or discussion\n\nI expect you to:\n\nRead the module material and homework assignment before coming to class\nEngage with the material and your classmates during class\nSeek help when you do not understand the material\nCommunicate promptly if you anticipate that you will have trouble meeting deadlines or participating in a portion of the course.\nDo your own troubleshooting before contacting me for help (and mention things you’ve already tried when you do ask for help!)\nBe respectful and considerate of everyone in the class\n\n\n3.4.1 Communication\nIf you are lost or confused or have a problem related to the course, make contact early! It is much easier to address a small problem early than to fix a cascade of issues later in the semester.\n\n\n3.4.2 Make Mistakes! 🚗 💥\nProgramming is the process of making a series of silly or stupid mistakes, and then slowly fixing each mistake (while potentially adding a few more). The only way to know how to fix these mistakes (and avoid them in the future) is to make them. (Sometimes, you have to make the same mistake a few dozen times before you can avoid it in the future). At some point during the class, you will find that you’ve spent 30 minutes staring at an error caused by a typo, a space, or a parenthesis in the wrong place. You may ask for help debugging this weird error, only to have someone immediately point out the problem… it is always easier to see these things in someone else’s code. This is part of programming, it is normal, and you shouldn’t feel embarrassed or sorry (unless you put no effort into troubleshooting the problem before you asked for help).\nIf you manage to produce an error I haven’t seen before, then congratulations. You have achieved something special, and that achievement should be celebrated. Each new and bizarre error is an opportunity to learn a bit more about the programming language, the operating system, or the interaction between the two.\n\n\n3.4.3 Evaluation Criteria\nIn every assignment, discussion, and written component of this class, you are expected to demonstrate that you are intellectually engaging with the material. I will evaluate you based on this engagement, which means that technically correct but low effort answers which do not demonstrate engagement or understanding will receive no credit.\nWhen you answer questions in this class, your goal is to show that you either understand the material or are actively engaging with it. If you did not achieve this goal, then your answer is incomplete, regardless of whether or not it is technically correct. This is not to encourage you to add unnecessary complexity to your answer - simple, elegant solutions are always preferable to unwieldy, complex solutions that accomplish the same task.\nGrammar and spelling are important, as is your ability to communicate technical information clearly in a written format; both of these criteria will be used in addition to assignment-specific rubrics to evaluate your work.\n\n\n3.4.4 Academic Integrity and Class Conduct\nYou will be engaging with your classmates and me through in-person discussions and collaborative activities. It is expected that everyone will engage in these interactions civilly and in good faith. Discussion and disagreement are important parts of the learning process, but it is important that mutual respect prevail. Individuals who detract from an atmosphere of civility and respect will be removed from the conversation or the classroom.\nStudents are expected to adhere to guidelines concerning academic dishonesty outlined in Article III B.1 of the University’s Student Code of Conduct. The Statistics Department academic integrity and grade appeal policy is available here.\n\nAI and Explainability Policy\nAny use of generative AI must be disclosed in an appendix to your submission - this includes brainstorming, editing, using AI as spell-check/grammar-check, and so on. You must document the following:\n\nthe version of the generative AI used\nthe full sequence of prompts and responses\nany additional inputs you provided to the AI system\na “diff” between the AI responses and your submission, showing exactly what was generated by the AI system and what you changed.\n\nIt may be useful to leverage AI tools to ensure that your work conforms to grammar and style guidelines, but I very highly discourage the use of generative AI for content or code.\n\n\nOral Exams\nI reserve the right to replace the grade on any written assignment with a graded oral exam on your submission. This policy is not intended to be punitive – it may be used to resolve issues where question wording was not clear or was misinterpreted. Rather, it is intended to be used to assess what you actually know when I suspect that the written assignment is not a good measure of your knowledge or skills.\n\n\nCode\nYou must be able to explain any work you turn in. - If you cannot explain the logic behind your approach as well as how it works in practice, then you will not receive credit for your submission. This is in line with what is generally considered acceptable behavior in programming - reuse is fine (subject to the code’s license), but you must still be able to understand and modify any code you did not write yourself.\n\nIf you copy and modify code from another source (StackOverflow, Rbloggers, etc.), provide that source in a comment above the code. This provides you with documentation as to how you got to the solution, and also provides credit to the original source. (This practice has been helpful to me on many occasions when I come back to code I wrote long ago)\n\n\n\nExams\nNo external resources are allowed on exams (including, but not limited to: the internet, AI, StackOverflow, a friend) for assistance unless specifically stated in the problem (and then, only on that specific problem). Use of any external resources will be grounds for an academic integrity violation under Section 2.A.5, “Failing to follow the rules”.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "Canvas\nReading Quizzes, Gradebook\n\n\n\nAssignment Submissions\n\n\nStatistical Computing in R and Python\nTextbook",
    "crumbs": [
      "Course information",
      "Useful links"
    ]
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "If you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "",
    "text": "If you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nOffice hours are available by appointment at this link. In general, office hours will be via Zoom - this allows me to offer more availability, and doesn’t require you to come to East campus to visit my office.\nIf you’d prefer to meet in person, try to schedule your appointment on Tuesday or Thursday, and mention in the request that you want an in-person meeting. I will let you know if that’s feasible given my schedule for the day.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, grades, accommodations, etc.). In these cases, please add “STAT351” to the subject line, so that I can prioritize your email.\nAssignment and content related questions should be asked in class or on Canvas.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times you may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Center for Academic Success and Transition (CAST). CAST offers free services to all students.\nStudents have access to peer tutoring, academic coaching, and academic success workshops.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nYour mental health and wellness extremely important, and the university offers resources to support students in managing daily stress and self-care. UNL offers several resources for students to manage stress, seek help with well-being, and access crisis and long-term care.\n\nCounseling and Psychological Services (CAPS), 402-472-7450\nCenter for Advocacy, Response, & Education (CARE), 402-472-3553 – resources for victims/survivors of interpersonal violence.\nServices for Students with Disabilities (SSD), ssd@unl.edu or 402-472-3787 – resources for students experiencing short or long-term disabilities that affect the campus or educational experience.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#technology",
    "href": "course-support.html#technology",
    "title": "Course support",
    "section": "Technology",
    "text": "Technology\nStudents who do not have a working computer can check out a loaner laptop from HuskerTech on city campus (Love South, Adele Hall) and east campus (Dinsdale).\nIf you are unable to get a loaner laptop set up with the software required for parts of this course, please try to let me know ahead of time - it is possible that I will be able to assist you with browser-based solutions.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#course-materials",
    "href": "course-support.html#course-materials",
    "title": "Course support",
    "section": "Course materials",
    "text": "Course materials\nThere are no costs associated with this course beyond the requirement that you have access to a laptop computer. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.) or library course reserves.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "weeks/week-02.html",
    "href": "weeks/week-02.html",
    "title": "Week 2: Web Scraping",
    "section": "",
    "text": "August 31-06, 2025\n\n\n\nWeb Scraping\n\n\n\n\n\nTry scraping one or more sites that are interesting to you.\n\n\n\n\n\nLab: Web Scraping\nLab goals\n\n\n\n\n\nLab: Web Scraping\n\n\n\n\n\nLab: Web Scraping\nLab goals",
    "crumbs": [
      "Weekly materials",
      "Week 2 - Web Scraping"
    ]
  },
  {
    "objectID": "weeks/week-02.html#reading",
    "href": "weeks/week-02.html#reading",
    "title": "Week 2: Web Scraping",
    "section": "",
    "text": "Web Scraping",
    "crumbs": [
      "Weekly materials",
      "Week 2 - Web Scraping"
    ]
  },
  {
    "objectID": "weeks/week-02.html#prepare-for-class",
    "href": "weeks/week-02.html#prepare-for-class",
    "title": "Week 2: Web Scraping",
    "section": "",
    "text": "Try scraping one or more sites that are interesting to you.",
    "crumbs": [
      "Weekly materials",
      "Week 2 - Web Scraping"
    ]
  },
  {
    "objectID": "weeks/week-02.html#tuesday",
    "href": "weeks/week-02.html#tuesday",
    "title": "Week 2: Web Scraping",
    "section": "",
    "text": "Lab: Web Scraping\nLab goals",
    "crumbs": [
      "Weekly materials",
      "Week 2 - Web Scraping"
    ]
  },
  {
    "objectID": "weeks/week-02.html#thursday",
    "href": "weeks/week-02.html#thursday",
    "title": "Week 2: Web Scraping",
    "section": "",
    "text": "Lab: Web Scraping",
    "crumbs": [
      "Weekly materials",
      "Week 2 - Web Scraping"
    ]
  },
  {
    "objectID": "weeks/week-02.html#practice-your-skills",
    "href": "weeks/week-02.html#practice-your-skills",
    "title": "Week 2: Web Scraping",
    "section": "",
    "text": "Lab: Web Scraping\nLab goals",
    "crumbs": [
      "Weekly materials",
      "Week 2 - Web Scraping"
    ]
  },
  {
    "objectID": "weeks/week-04.html",
    "href": "weeks/week-04.html",
    "title": "Week 4: Application Programming Interfaces",
    "section": "",
    "text": "September 14-20, 2025\n\n\n\nApplication Programming Interfaces (APIs)\n\n\n\n\n\n\n\n\n\n\n\nLecture: API requests\n\n\n\n\n\nLab: APIs\n\n\n\n\n\nLab: APIs\nLab goals",
    "crumbs": [
      "Weekly materials",
      "Week 4 - APIs"
    ]
  },
  {
    "objectID": "weeks/week-04.html#reading",
    "href": "weeks/week-04.html#reading",
    "title": "Week 4: Application Programming Interfaces",
    "section": "",
    "text": "Application Programming Interfaces (APIs)",
    "crumbs": [
      "Weekly materials",
      "Week 4 - APIs"
    ]
  },
  {
    "objectID": "weeks/week-04.html#tuesday",
    "href": "weeks/week-04.html#tuesday",
    "title": "Week 4: Application Programming Interfaces",
    "section": "",
    "text": "Lecture: API requests",
    "crumbs": [
      "Weekly materials",
      "Week 4 - APIs"
    ]
  },
  {
    "objectID": "weeks/week-04.html#thursday",
    "href": "weeks/week-04.html#thursday",
    "title": "Week 4: Application Programming Interfaces",
    "section": "",
    "text": "Lab: APIs",
    "crumbs": [
      "Weekly materials",
      "Week 4 - APIs"
    ]
  },
  {
    "objectID": "weeks/week-04.html#practice-your-skills",
    "href": "weeks/week-04.html#practice-your-skills",
    "title": "Week 4: Application Programming Interfaces",
    "section": "",
    "text": "Lab: APIs\nLab goals",
    "crumbs": [
      "Weekly materials",
      "Week 4 - APIs"
    ]
  },
  {
    "objectID": "weeks/week-06.html",
    "href": "weeks/week-06.html",
    "title": "Week 6: Using Appropriate Graphics",
    "section": "",
    "text": "September 28-04, 2025\n\n\n\nAppropriate Graphics\n\n\n\n\n\nFind the worst data visualization published by a news organization within the last month and bring it with you to class.\n\n\n\n\n\nLecture: Appropriate Graphics\n\n\n\n\n\nLab: Appropriate Graphics\n\n\n\n\n\nLab: Appropriate Graphics\nLab goals",
    "crumbs": [
      "Weekly materials",
      "Week 6 - Appropriate Graphics"
    ]
  },
  {
    "objectID": "weeks/week-06.html#reading",
    "href": "weeks/week-06.html#reading",
    "title": "Week 6: Using Appropriate Graphics",
    "section": "",
    "text": "Appropriate Graphics",
    "crumbs": [
      "Weekly materials",
      "Week 6 - Appropriate Graphics"
    ]
  },
  {
    "objectID": "weeks/week-06.html#prepare-for-class",
    "href": "weeks/week-06.html#prepare-for-class",
    "title": "Week 6: Using Appropriate Graphics",
    "section": "",
    "text": "Find the worst data visualization published by a news organization within the last month and bring it with you to class.",
    "crumbs": [
      "Weekly materials",
      "Week 6 - Appropriate Graphics"
    ]
  },
  {
    "objectID": "weeks/week-06.html#tuesday",
    "href": "weeks/week-06.html#tuesday",
    "title": "Week 6: Using Appropriate Graphics",
    "section": "",
    "text": "Lecture: Appropriate Graphics",
    "crumbs": [
      "Weekly materials",
      "Week 6 - Appropriate Graphics"
    ]
  },
  {
    "objectID": "weeks/week-06.html#thursday",
    "href": "weeks/week-06.html#thursday",
    "title": "Week 6: Using Appropriate Graphics",
    "section": "",
    "text": "Lab: Appropriate Graphics",
    "crumbs": [
      "Weekly materials",
      "Week 6 - Appropriate Graphics"
    ]
  },
  {
    "objectID": "weeks/week-06.html#practice-your-skills",
    "href": "weeks/week-06.html#practice-your-skills",
    "title": "Week 6: Using Appropriate Graphics",
    "section": "",
    "text": "Lab: Appropriate Graphics\nLab goals",
    "crumbs": [
      "Weekly materials",
      "Week 6 - Appropriate Graphics"
    ]
  },
  {
    "objectID": "weeks/week-08.html",
    "href": "weeks/week-08.html",
    "title": "Week 8: Interactive JavaScript Graphics",
    "section": "",
    "text": "October 12-18, 2025\n\n\n\nInteractive Graphics (Plotly)\nObservable JavaScript primer\nObservable from ggplot2\n\n\n\n\n\nWatch John Tukey demonstrate PRIM-9, a graphics program for an early computer\nWatch Andreas Buja talk about Linked Graphics\n\n\n\n\n\nLecture: Interactive Graphics using Plotly and Observable\n\n\n\n\n\nLab: Interactive Graphics with Plotly and Observable\n\n\n\n\n\nLab: Interactive Graphics with Plotly and Observable\nLab goals",
    "crumbs": [
      "Weekly materials",
      "Week 8 - JavaScript Graphics"
    ]
  },
  {
    "objectID": "weeks/week-08.html#reading",
    "href": "weeks/week-08.html#reading",
    "title": "Week 8: Interactive JavaScript Graphics",
    "section": "",
    "text": "Interactive Graphics (Plotly)\nObservable JavaScript primer\nObservable from ggplot2",
    "crumbs": [
      "Weekly materials",
      "Week 8 - JavaScript Graphics"
    ]
  },
  {
    "objectID": "weeks/week-08.html#prepare-for-class",
    "href": "weeks/week-08.html#prepare-for-class",
    "title": "Week 8: Interactive JavaScript Graphics",
    "section": "",
    "text": "Watch John Tukey demonstrate PRIM-9, a graphics program for an early computer\nWatch Andreas Buja talk about Linked Graphics",
    "crumbs": [
      "Weekly materials",
      "Week 8 - JavaScript Graphics"
    ]
  },
  {
    "objectID": "weeks/week-08.html#tuesday",
    "href": "weeks/week-08.html#tuesday",
    "title": "Week 8: Interactive JavaScript Graphics",
    "section": "",
    "text": "Lecture: Interactive Graphics using Plotly and Observable",
    "crumbs": [
      "Weekly materials",
      "Week 8 - JavaScript Graphics"
    ]
  },
  {
    "objectID": "weeks/week-08.html#thursday",
    "href": "weeks/week-08.html#thursday",
    "title": "Week 8: Interactive JavaScript Graphics",
    "section": "",
    "text": "Lab: Interactive Graphics with Plotly and Observable",
    "crumbs": [
      "Weekly materials",
      "Week 8 - JavaScript Graphics"
    ]
  },
  {
    "objectID": "weeks/week-08.html#practice-your-skills",
    "href": "weeks/week-08.html#practice-your-skills",
    "title": "Week 8: Interactive JavaScript Graphics",
    "section": "",
    "text": "Lab: Interactive Graphics with Plotly and Observable\nLab goals",
    "crumbs": [
      "Weekly materials",
      "Week 8 - JavaScript Graphics"
    ]
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10: Approaches to Big Data",
    "section": "",
    "text": "October 26-01, 2025\n\n\n\nLecture: Computers and Data Storage Trade-Offs",
    "crumbs": [
      "Weekly materials",
      "Week 10 - Big Data"
    ]
  },
  {
    "objectID": "weeks/week-10.html#tuesday",
    "href": "weeks/week-10.html#tuesday",
    "title": "Week 10: Approaches to Big Data",
    "section": "",
    "text": "Lecture: Computers and Data Storage Trade-Offs",
    "crumbs": [
      "Weekly materials",
      "Week 10 - Big Data"
    ]
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 12: Apache Arrow",
    "section": "",
    "text": "Week 12: Apache Arrow\nNovember 09-15, 2025",
    "crumbs": [
      "Weekly materials",
      "Week 12 - Arrow"
    ]
  },
  {
    "objectID": "weeks/week-14.html",
    "href": "weeks/week-14.html",
    "title": "Week 14: Thanksgiving",
    "section": "",
    "text": "November 23-29, 2025\n\n\n\nNo Class! Have a nice break!\n\n\n\n\n\nHappy Turkey Day! :turkey:"
  },
  {
    "objectID": "weeks/week-14.html#tuesday",
    "href": "weeks/week-14.html#tuesday",
    "title": "Week 14: Thanksgiving",
    "section": "",
    "text": "No Class! Have a nice break!"
  },
  {
    "objectID": "weeks/week-14.html#thursday",
    "href": "weeks/week-14.html#thursday",
    "title": "Week 14: Thanksgiving",
    "section": "",
    "text": "Happy Turkey Day! :turkey:"
  },
  {
    "objectID": "weeks/week-16.html",
    "href": "weeks/week-16.html",
    "title": "Week 16: Special Topics",
    "section": "",
    "text": "Week 16: Special Topics\nDecember 07-13, 2025",
    "crumbs": [
      "Weekly materials",
      "Week 16 - Special Topics"
    ]
  },
  {
    "objectID": "assignment-repos/01-web-scraping/index.html",
    "href": "assignment-repos/01-web-scraping/index.html",
    "title": "Lab: Web Scraping and RSS",
    "section": "",
    "text": "The Securities and Exchange Commission (SEC) provides a way to see the most recent filings submitted to the commission at https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent.\nForm 144 is required to be submitted when an executive officer, director, or affiliate of the company sells more than 5000 shares of stock or the aggregate sales price exceeds $50,000.\nConveniently, we can access the 100 most recent Form 144 filings by using the RSS feed, which provides some basic information about each filing as well as a link to the filing’s database entry. An example of the RSS feed downloaded on July 28, 2025 is provided in the sample-feed.rss file for demonstration purposes.\nRSS feeds are just another XML-like file, as you can see from the RSS2.0 Specification."
  },
  {
    "objectID": "assignment-repos/01-web-scraping/index.html#parsing-the-file",
    "href": "assignment-repos/01-web-scraping/index.html#parsing-the-file",
    "title": "Lab: Web Scraping and RSS",
    "section": "1.1 Parsing the File",
    "text": "1.1 Parsing the File\nAdd a code chunk that will read in the RSS feed as an XML file."
  },
  {
    "objectID": "assignment-repos/01-web-scraping/index.html#sec-extract-info",
    "href": "assignment-repos/01-web-scraping/index.html#sec-extract-info",
    "title": "Lab: Web Scraping and RSS",
    "section": "1.2 Extracting Information",
    "text": "1.2 Extracting Information\nAdd a code chunk that will convert each entry into a data frame row with columns title, link, summary, updated, category, and id. Note that you may need to access attributes to get the important data out of some nodes, while for others you may only need to access the content.\nShow the first 5 and last 5 rows of your data, using a function like knitr::kable, DT::DT, or IPython’s display() and Markdown(). See this page for more information about rendering “pretty” tabular data using quarto."
  },
  {
    "objectID": "assignment-repos/01-web-scraping/index.html#is-it-xml",
    "href": "assignment-repos/01-web-scraping/index.html#is-it-xml",
    "title": "Lab: Web Scraping and RSS",
    "section": "1.3 Is it XML?",
    "text": "1.3 Is it XML?\nIs an RSS feed actually an XML file? Why or why not? What requirements of an XML file does an RSS feed meet, and what requirements are lacking?"
  },
  {
    "objectID": "assignment-repos/01-web-scraping/index.html#tidy-data",
    "href": "assignment-repos/01-web-scraping/index.html#tidy-data",
    "title": "Lab: Web Scraping and RSS",
    "section": "2.1 Tidy Data",
    "text": "2.1 Tidy Data\nWhat aspects of the data frame need work in order to meet the criteria for tidy data?"
  },
  {
    "objectID": "assignment-repos/01-web-scraping/index.html#tidy-planning",
    "href": "assignment-repos/01-web-scraping/index.html#tidy-planning",
    "title": "Lab: Web Scraping and RSS",
    "section": "2.2 Tidy Planning",
    "text": "2.2 Tidy Planning\nWrite a step-by-step plan to convert your data into tidy form."
  },
  {
    "objectID": "assignment-repos/01-web-scraping/index.html#tidying-for-real",
    "href": "assignment-repos/01-web-scraping/index.html#tidying-for-real",
    "title": "Lab: Web Scraping and RSS",
    "section": "2.3 Tidying, for real!",
    "text": "2.3 Tidying, for real!\nExecute your step-by-step plan. Show a nicely formatted data table with up to 10 entries, and explain why your data is now tidy."
  },
  {
    "objectID": "assignment-repos/01-web-scraping/index.html#preparing",
    "href": "assignment-repos/01-web-scraping/index.html#preparing",
    "title": "Lab: Web Scraping and RSS",
    "section": "3.1 Preparing",
    "text": "3.1 Preparing\nOpen one link from your table and examine the HTML page.\n\nIdentify the CSS selector you believe would be most efficient to obtain the table of document format files.\n\n\nYour answer here\n\n\nIdentify the CSS selector you believe would be most efficient for obtaining the first link in the table of document format files.\n\n\nYour answer here"
  },
  {
    "objectID": "assignment-repos/01-web-scraping/index.html#reading-html-tables",
    "href": "assignment-repos/01-web-scraping/index.html#reading-html-tables",
    "title": "Lab: Web Scraping and RSS",
    "section": "3.2 Reading HTML Tables",
    "text": "3.2 Reading HTML Tables\nWrite a function that takes a URI to the Filing Detail page and extracts the link for the document corresponding to the filing entry (I recommend going for the .htm link). Use your function to obtain the HTML addresses of the filed forms for all of the links in your RSS feed."
  },
  {
    "objectID": "assignment-repos/01-web-scraping/index.html#reading-html-pages",
    "href": "assignment-repos/01-web-scraping/index.html#reading-html-pages",
    "title": "Lab: Web Scraping and RSS",
    "section": "3.3 Reading HTML Pages",
    "text": "3.3 Reading HTML Pages\nUse the links to the filed forms and extract the following information from each page, storing the information for each form in a data frame.\n\nName, SEC File Number, Address, and Phone Number of Issuer\nName of person selling the securities, and relationship to the Issuer\nSecurities Information: all values in the table\nSecurities to be Sold: all values in the table\nOther sales in the last 3 months, if any\nNotice date\n\nYou may want to write one or more functions to extract the necessary data, with error handling in order to ensure that if the data does not exist the function still returns the information that is available."
  },
  {
    "objectID": "assignment-repos/01-web-scraping/index.html#assessing-market-value-of-stock-sales",
    "href": "assignment-repos/01-web-scraping/index.html#assessing-market-value-of-stock-sales",
    "title": "Lab: Web Scraping and RSS",
    "section": "3.4 Assessing Market Value of Stock Sales",
    "text": "3.4 Assessing Market Value of Stock Sales\nMake a histogram of the aggregate market value of stock to be sold for the entries in your RSS feed. Make sure your plot has descriptive titles, axis labels, and uses appropriate scales."
  },
  {
    "objectID": "assignment-repos/01-web-scraping/index.html#efficiency",
    "href": "assignment-repos/01-web-scraping/index.html#efficiency",
    "title": "Lab: Web Scraping and RSS",
    "section": "3.5 Efficiency",
    "text": "3.5 Efficiency\nImagine you’d saved the RSS feed from the SEC for the past 30 days. The SEC may not receive more than 100 form 144 filings each day, and they do not update the feed on federal holidays or weekends.\nDescribe how you would approach generating a database of the unique form 144 filings over all 30 days of RSS feed files. Make an ordered list of steps indicating the order in which you would read in the RSS entries, acquire the corresponding data from the SEC filing form, and deduplicating the potentially repeated filings, along with any other intermediate steps you might take.\nYour solution should minimize the amount of load on the SEC’s server both because it minimizes the running time of the code and because it’s more polite.\nCalculate the total number of calls you have to make to the SEC’s server to implement your solution, showing your work."
  },
  {
    "objectID": "assignment-repos/02-list-processing/index.html",
    "href": "assignment-repos/02-list-processing/index.html",
    "title": "Lab: List Processing",
    "section": "",
    "text": "You can see the purpose of this assignment as well as the skills and knowledge you should be using and acquiring, in the Transparency in Learning and Teaching (TILT) document in this repository. The TILT document also contains a checklist for self-reflection that will provide some guidance on how the assignment will be graded."
  },
  {
    "objectID": "assignment-repos/02-list-processing/index.html#parse-the-file",
    "href": "assignment-repos/02-list-processing/index.html#parse-the-file",
    "title": "Lab: List Processing",
    "section": "2.1 Parse the file",
    "text": "2.1 Parse the file\nAdd a code chunk that will read each of the JSON files in. Store the data in a drwhoYYYY object, where YYYY is the first year the series began to air. How are the data objects stored?"
  },
  {
    "objectID": "assignment-repos/02-list-processing/index.html#examining-list-data-structures",
    "href": "assignment-repos/02-list-processing/index.html#examining-list-data-structures",
    "title": "Lab: List Processing",
    "section": "2.2 Examining List Data Structures",
    "text": "2.2 Examining List Data Structures\nCreate a nested markdown list showing what variables are nested at each level of the JSON file. Include an ‘episode’ object that is a stand-in for a generic episode (e.g. don’t create a list with all 700+ episodes in it, just show what a single episode has). Make sure you use proper markdown formatting to ensure that the lists are rendered properly when you compile your document.\nHint: The prettify() function in the R package jsonlite will spit out a better-formatted version of a JSON file.\n\nList here\n\nIs there any information stored in the list structure that you feel is redundant? If so, why?"
  },
  {
    "objectID": "assignment-repos/02-list-processing/index.html#develop-a-strategy",
    "href": "assignment-repos/02-list-processing/index.html#develop-a-strategy",
    "title": "Lab: List Processing",
    "section": "2.3 Develop A Strategy",
    "text": "2.3 Develop A Strategy\nConsider what information you would need to examine the structure of Dr. Who episodes over time (show runtime, season length, specials) as well as the ratings, combining information across all three data files.\nSketch one or more rectangular data tables that look like your expected output. Remember that if you link to an image, you must link to something with a picture extension (.png, .jpg), and if you reference a file it should be using a local path and you must also add the picture to your git repository.\n\nSketch goes here\n\nWhat operations will you need to perform to get the data into a form matching your sketch? Make an ordered list of steps you need to take.\n\n\n… fill me in\n…\n…"
  },
  {
    "objectID": "assignment-repos/02-list-processing/index.html#implement-your-strategy",
    "href": "assignment-repos/02-list-processing/index.html#implement-your-strategy",
    "title": "Lab: List Processing",
    "section": "2.4 Implement Your Strategy",
    "text": "2.4 Implement Your Strategy\nAdd a code chunk that will convert the JSON files into the table(s) you sketched above. Make sure that the resulting tables have the correct variable types (e.g., dates should not be stored as character variables).\nPrint out the first 5 rows of each table that you create (but no more)!"
  },
  {
    "objectID": "assignment-repos/02-list-processing/index.html#examining-episode-air-dates",
    "href": "assignment-repos/02-list-processing/index.html#examining-episode-air-dates",
    "title": "Lab: List Processing",
    "section": "2.5 Examining Episode Air Dates",
    "text": "2.5 Examining Episode Air Dates\nVisually represent the length of time between air dates of adjacent episodes within the same season, across all seasons of Dr. Who. You may need to create a factor to indicate which Dr. Who series is indicated, as there will be a Season 1 for each of the series. Your plot must have appropriate labels and a title.\n\ncode chunk here\n\nIn 2-3 sentences, explain what conclusions you might draw from the data. What patterns do you notice? Are there data quality issues?"
  },
  {
    "objectID": "assignment-repos/02-list-processing/index.html#setting-up",
    "href": "assignment-repos/02-list-processing/index.html#setting-up",
    "title": "Lab: List Processing",
    "section": "3.1 Setting Up",
    "text": "3.1 Setting Up\nIn this section of the assignment, you will work with all of the provided JSON files. Use a functional programming approach to read in all of the files and bind them together.\n\nfunctional code goes here\n\nThen, use the processing code you wrote for the previous section to perform appropriate data cleaning steps. At the end of the chunk, your data should be in a reasonably tidy, rectangular form with appropriate data types. Call this rectangular table whoverse.\n\nCleaning code goes here"
  },
  {
    "objectID": "assignment-repos/02-list-processing/index.html#air-time",
    "href": "assignment-repos/02-list-processing/index.html#air-time",
    "title": "Lab: List Processing",
    "section": "3.2 Air Time",
    "text": "3.2 Air Time\nInvestigate the air time of the episodes relative to the air date, series, and season. It may help to know that the watershed period in the UK is 9:00pm - 5:30am. Content that is unsuitable for minors may only be shown during this window. What conclusions do you draw about the target audience for each show?\nHow can you explain any shows in the Dr. Who universe which do not have airtimes provided?"
  },
  {
    "objectID": "assignment-repos/02-list-processing/index.html#another-layer-of-json",
    "href": "assignment-repos/02-list-processing/index.html#another-layer-of-json",
    "title": "Lab: List Processing",
    "section": "3.3 Another Layer of JSON",
    "text": "3.3 Another Layer of JSON\nUse the show URL (_links &gt; show &gt; href) to read in the JSON file for each show. As with scraping, it is important to be polite and not make unnecessary server calls, so pre-process the data to ensure that you only make one server call for each show. You should use a functional programming approach when reading in these files.\n\nRead in JSON files from URLs here\n\nProcess the JSON files using a functional approach and construct an appropriate table for the combined data you’ve acquired during this step (no need to join the data with the full whoverse episode-level data).\n\nProcess JSON files to make a table here\n\nWhat keys would you use to join this data with the whoverse episode level data? Explain.\n\nExplanation"
  },
  {
    "objectID": "assignment-repos/02-list-processing/index.html#explore",
    "href": "assignment-repos/02-list-processing/index.html#explore",
    "title": "Lab: List Processing",
    "section": "3.4 Explore!",
    "text": "3.4 Explore!\nUse the data you’ve assembled to answer a question you find interesting about this data. Any graphics you make should have appropriate titles and axis labels. Tables should be reasonably concise (e.g. don’t show all 900 episodes in a table), generated in a reproducible fashion, and formatted with markdown. Any results (graphics, tables, models) should be explained with at least 2-3 sentences.\nIf you’re stuck, consider examining the frequency of words in the episode descriptions across different series or seasons. Or, look at the episode guest cast by appending /guestcast/ to the episode URL and see whether there are common guests across different seasons.\n\nQuestion goes here\n\nCode goes here – once you output a result, you should explain it using markdown text, and then start a new code chunk to continue your exploration."
  },
  {
    "objectID": "assignment-repos/04-apis/index.html",
    "href": "assignment-repos/04-apis/index.html",
    "title": "Lab: Application Programming Interfaces",
    "section": "",
    "text": "You can see the purpose of this assignment as well as the skills and knowledge you should be using and acquiring, in the Transparency in Learning and Teaching (TILT) document in this repository. The TILT document also contains a checklist for self-reflection that will provide some guidance on how the assignment will be graded."
  },
  {
    "objectID": "assignment-repos/04-apis/index.html#http-response-codes",
    "href": "assignment-repos/04-apis/index.html#http-response-codes",
    "title": "Lab: Application Programming Interfaces",
    "section": "1.1 HTTP Response Codes",
    "text": "1.1 HTTP Response Codes\nScrape the HTTP response descriptions provided at https://www.ibm.com/docs/en/zos-connect/3.0.0?topic=reference-http-response-code into a tabular object resp_codes that has columns code, name, and description.\n\nCode chunk goes here"
  },
  {
    "objectID": "assignment-repos/04-apis/index.html#fighting-like-cats-and-dogs-with-apis",
    "href": "assignment-repos/04-apis/index.html#fighting-like-cats-and-dogs-with-apis",
    "title": "Lab: Application Programming Interfaces",
    "section": "1.2 Fighting Like Cats and Dogs with APIs",
    "text": "1.2 Fighting Like Cats and Dogs with APIs\nOften, 404 errors have custom pages that show an image specific to the site (or just more interesting than a text site that says 404 not found). There are API services available to provide these images, and using these services to show response codes can make web development and web scraping much cuter.\nFor this exercise, pick either dogs or cats. (You can also pick goats, ducks, gardens, fish, or pizza, if you prefer.) Complete the following tasks to make a reference sheet for each of your HTTP codes.\n\nSaving the response images\n\nUse the code column and the API documentation to create a URL for each status code picture. Save the URLs in a url column.\nCreate a folder to save the images into (cats/, dogs/, pizza/, etc. seems reasonable)\nUse functional programming to iterate through your url column and download the pictures to your folder, saving each image as XXX.jpg where XXX is the HTTP response code.\n\n\n\nIntroducing glue and f-strings\n\nPython f-string reference\nglue R package reference\n\nAs API calls get more complicated, it can be helpful to have a way to specify structured strings without pasting a bunch of stuff together. To practice this, use your language of choice to generate markdown code to display each of the HTTP response code images you downloaded.\nEach image should have:\n\na caption\nalt-text\nan image ID\n\nAs a reminder, you can accomplish this using the following markdown syntax:\n ![&lt;caption&gt;](&lt;path-to-image&gt;){#&lt;id&gt; fig-alt=\"alt-text to explain what the image is\"}\nCreate a formatted string that can be combined with your data to generate a complete image reference string. Indicate which columns in your data will be filled in to each placeholder in the string. Make sure to use your local image link when generating markdown image code, to reduce the load on the server.\n\n\nCreating an Image Wall\nUse your strings from the previous problem to generate a quarto file that will display a wall of images by following these steps in order:\n\nGenerate markdown code for each HTTP response code by filling in the placeholders\nCreate a YAML header string to provide the information at the top of your quarto document, including a title, author, date, and output format (html or pdf).\nCreate a start and end string that will define a figure panel.\nAssemble your image codes into a single string, keeping in mind that there must be a blank line between each image for the figure panel syntax to work properly.\nAssemble the lines you have created into a coherent vector that will define the lines of a text file (e.g. YAMl, then start, then image codes, then end)\nWrite the lines out to http-codes.qmd.\nCompile your http-codes.qmd file and ensure that it works! You can use the quarto::render() function if you want to do this within this document.\n\n\n\n# Step 1\n\n# Step 2\n\n# Step 3\n\n# Step 4\n\n# Step 5\n\n# Step 6\n\n# Step 7"
  },
  {
    "objectID": "assignment-repos/04-apis/index.html#register-for-calendarific",
    "href": "assignment-repos/04-apis/index.html#register-for-calendarific",
    "title": "Lab: Application Programming Interfaces",
    "section": "2.1 Register for Calendarific",
    "text": "2.1 Register for Calendarific\nThe Calendarific API (https://calendarific.com/api-documentation) allows 500 - 1000 requests per day for free accounts. Register for a free account (you can use your GitHub login for simplicity) and acquire your API key.\nUse an environment file (.Renviron or .env) to store your API key in an environment variable named CALENDARIFIC_API_KEY. Add your environment file to the .gitignore file in this repository to ensure that you do not accidentally push it to git, exposing your API key to the world. Add the .gitignore file and push it to the repository.\nTake a screenshot of your working directory to show that you’ve created your environment file correctly, and include it here.\n\nScreenshot goes here\n\nAlso, load your environment variables (restart your R session, load the python-dotenv package, etc.) and determine how many characters are in your API key. Include the code you used to do this, but set eval: false as an execution option for the chunk.\n\nChunk goes here\n\n\nThere are … characters in my API key\n\nWhen this homework is compiled on my machine, I will use my own API key, constructed in the same way, to run your code. If you did not follow the instructions exactly, it will not work."
  },
  {
    "objectID": "assignment-repos/04-apis/index.html#october-holidays",
    "href": "assignment-repos/04-apis/index.html#october-holidays",
    "title": "Lab: Application Programming Interfaces",
    "section": "2.2 October Holidays",
    "text": "2.2 October Holidays\nRead the API documentation and construct a query that will get all US holidays in October for this calendar year. Run the query in R or Python and store the query response in an oct_holidays object. Write out the response to a file and commit it to the repository. Wrap your API call in code which will check for the presence of the file and only make an API call if the file does not exist.\nNote: An .Rdata file is fine if you’re using R. I’ve included save-object.py to write out an object in a binary file in Python. It’s important to not write out your request to a plain-text readable format because the request will likely contain your API key, which you want to keep confidential.\n\n\nCode here\n\n\nWhy is it important to minimize redundant API calls to the server by saving the request file?\n\nYour answer"
  },
  {
    "objectID": "assignment-repos/04-apis/index.html#formatting-json-data",
    "href": "assignment-repos/04-apis/index.html#formatting-json-data",
    "title": "Lab: Application Programming Interfaces",
    "section": "2.3 Formatting JSON Data",
    "text": "2.3 Formatting JSON Data\nAcquire the request response body as a JSON file. Save the JSON file to your directory as oct-holidays.json.\nUse your data tidying skills to format the data in a rectangular format with minimal nesting (you can allow nesting in the states variables to keep the data in a relatively compact form). Dates should be stored as dates and not characters.\nHint: When you are finished, your data should have at least the columns name, description, country_id, country_name, date, type, canonical_url, locations, states, and of these, only states should be a complex type."
  },
  {
    "objectID": "assignment-repos/04-apis/index.html#state-holidays",
    "href": "assignment-repos/04-apis/index.html#state-holidays",
    "title": "Lab: Application Programming Interfaces",
    "section": "2.4 State Holidays",
    "text": "2.4 State Holidays\nCreate separate data structures for holidays which are celebrated in all US states, and holidays which are celebrated only in some states. Unnest the states column in each data set before merging them back together (you may want to rename the states column in the national holiday dataset to more appropriately match the state dataset).\n\nCode"
  },
  {
    "objectID": "assignment-repos/04-apis/index.html#holiday-list",
    "href": "assignment-repos/04-apis/index.html#holiday-list",
    "title": "Lab: Application Programming Interfaces",
    "section": "2.5 Holiday List",
    "text": "2.5 Holiday List\nUsing this combined dataset, create a properly formatted markdown table of all state, national, or religious holidays which are celebrated either nationally or within Nebraska during October, sorted by date (include only necessary columns - name, date, type). Include only one entry for each holiday – if something is both a national and local holiday, include only the national version. You should exclude UN observances (other than World Statistics Day!), Sporting events, and Worldwide observances.\n\nCode that outputs the markdown table"
  },
  {
    "objectID": "assignment-repos/04-apis/index.html#date-sequences",
    "href": "assignment-repos/04-apis/index.html#date-sequences",
    "title": "Lab: Application Programming Interfaces",
    "section": "2.6 Date Sequences",
    "text": "2.6 Date Sequences\nSome holidays occur over a longer period – for example, the Jewish holiday of Sukkot lasts for about a week. Calenderific includes the first and last days of Sukkot, but does not include the intermediate period. Write a function, long_holiday(data, \"name\") that looks for “First Day of ” and “Last Day of ” entries in data$name and fills in the additional days, labeling all of the days with the holiday name (“”). Test your function to ensure that it works for October."
  },
  {
    "objectID": "assignment-repos/04-apis/index.html#scheduling-an-event",
    "href": "assignment-repos/04-apis/index.html#scheduling-an-event",
    "title": "Lab: Application Programming Interfaces",
    "section": "2.7 Scheduling An Event",
    "text": "2.7 Scheduling An Event\nSuppose you want to schedule an event to occur during the fall semester. You want to be sensitive to religious holidays (you don’t have to decide whether a holiday is important enough for someone to miss the event to celebrate, which gets sticky), as well as events like time changes and national/state holidays. In addition, you also want to be sensitive to those who are religious, so you want to avoid holding your event on Saturday or Sunday. ()) + 1 Request all holidays and determine what the safe dates are to hold your event. You may hard-code the dates when classes are in session for the semester, or just use the values directly in a filter statement.\nShow the safe days using an appropriate plot. The calendR package may be useful, or you can use lubridate to get the necessary information to plot the calendar in ggplot2."
  },
  {
    "objectID": "assignment-repos/05-pdf-parsing/index.html",
    "href": "assignment-repos/05-pdf-parsing/index.html",
    "title": "Lab: PDF Data",
    "section": "",
    "text": "You can see the purpose of this assignment as well as the skills and knowledge you should be using and acquiring, in the Transparency in Learning and Teaching (TILT) document in this repository. The TILT document also contains a checklist for self-reflection that will provide some guidance on how the assignment will be graded."
  },
  {
    "objectID": "assignment-repos/05-pdf-parsing/index.html#pdf-format",
    "href": "assignment-repos/05-pdf-parsing/index.html#pdf-format",
    "title": "Lab: PDF Data",
    "section": "2.1 PDF Format",
    "text": "2.1 PDF Format\nWhat type of PDF files are these? Based on the format, what would you have to do to process the PDFs and extract text and/or tabular information (in broad terms)?\n\nyour answer here"
  },
  {
    "objectID": "assignment-repos/05-pdf-parsing/index.html#pdf-structure",
    "href": "assignment-repos/05-pdf-parsing/index.html#pdf-structure",
    "title": "Lab: PDF Data",
    "section": "2.2 PDF Structure",
    "text": "2.2 PDF Structure\nTake a look at the provided PDFs in your default PDF viewer (acrobat, reader, chrome, xPDF, etc). Do they have a consistent structure across years? Across departments? Make a list of at least 3-5 problems you expect to have to overcome if you scrape data from the PDFs. Include screenshots where it is relevant to do so (similar to Fig 34.4 in the textbook), and make sure your images are included using appropriate markdown syntax, captions, and hyperlinks. Discuss how you might overcome these problems and why the PDF format leads to processing challenges.\n\nList here\n\n\nSupporting screenshots/discussion here"
  },
  {
    "objectID": "assignment-repos/05-pdf-parsing/index.html#plan-of-attack",
    "href": "assignment-repos/05-pdf-parsing/index.html#plan-of-attack",
    "title": "Lab: PDF Data",
    "section": "2.3 Plan of Attack",
    "text": "2.3 Plan of Attack\nWhat strategy would you use to read in the budget data to minimize the amount of post-processing you need to do? Explain your reasoning.\n\n\nStrategy here\n\n\nExplanation"
  },
  {
    "objectID": "assignment-repos/05-pdf-parsing/index.html#acquiring-metadata",
    "href": "assignment-repos/05-pdf-parsing/index.html#acquiring-metadata",
    "title": "Lab: PDF Data",
    "section": "2.4 Acquiring Metadata",
    "text": "2.4 Acquiring Metadata\nUse a PDF library to programmatically examine each PDF. Use a functional approach, and organize the metadata in a table, with one row per file. Do you notice any anomalies or unfamiliar metadata components which might be important? Propose a possible hypothesis for any anomalies you discover, and research/explain any unfamiliar terms in the metadata that you identified.\n\n\nCode chunk should replace this line\n\n\n\nYour response to open-ended questions goes here"
  },
  {
    "objectID": "assignment-repos/05-pdf-parsing/index.html#anomalies-and-strategy-adjustments",
    "href": "assignment-repos/05-pdf-parsing/index.html#anomalies-and-strategy-adjustments",
    "title": "Lab: PDF Data",
    "section": "2.5 Anomalies and Strategy Adjustments",
    "text": "2.5 Anomalies and Strategy Adjustments\nConsidering what you discovered in the previous step, do you need to adjust your strategy for reading in the data? Why or why not? Investigate any differences in metadata values across files, and determine whether or not the variation(s) may pose problems for your analysis.\n\n\nCode chunk should replace this line (delete line and break if not needed)\n\n\n\nYour response to open-ended questions goes here"
  },
  {
    "objectID": "assignment-repos/05-pdf-parsing/index.html#identify-relevant-pages",
    "href": "assignment-repos/05-pdf-parsing/index.html#identify-relevant-pages",
    "title": "Lab: PDF Data",
    "section": "3.1 Identify Relevant Pages",
    "text": "3.1 Identify Relevant Pages\nDevelop a function that takes the path to a PDF file and identifies which pages have tabular salary data on them (e.g. get a range of pages with the salary information). Use your function to create a table with columns file_name, page_start, and page_end.\n\n\nCode Chunk"
  },
  {
    "objectID": "assignment-repos/05-pdf-parsing/index.html#read-in-relevant-text",
    "href": "assignment-repos/05-pdf-parsing/index.html#read-in-relevant-text",
    "title": "Lab: PDF Data",
    "section": "3.2 Read in Relevant Text",
    "text": "3.2 Read in Relevant Text\nDevelop a function read_salary_data(file, start, end) which will read in all of the salary data from the pages with tables, using the 2025-2026 salary report as a guide. You should not generalize to other years yet. Use the pdf_text function in tabulapdf (R) or the read_pdf function in the tabula-py package (python).\n\n\nCode Chunk"
  },
  {
    "objectID": "assignment-repos/05-pdf-parsing/index.html#plan-your-approach",
    "href": "assignment-repos/05-pdf-parsing/index.html#plan-your-approach",
    "title": "Lab: PDF Data",
    "section": "3.3 Plan your Approach",
    "text": "3.3 Plan your Approach\nWhat processing steps would you use to get the text vectors from this function into a table? Make a detailed list of the necessary steps. Are there any steps you do not think will be consistently successful or generalizable? Is there information your steps sacrifice to read things in cleanly?\n\n\nYour answer should include an ordered list of steps (indent with at least 2 spaces to form a nested list, if necessary), as well as a response to the open-ended questions at the end."
  },
  {
    "objectID": "assignment-repos/05-pdf-parsing/index.html#would-coordinates-help",
    "href": "assignment-repos/05-pdf-parsing/index.html#would-coordinates-help",
    "title": "Lab: PDF Data",
    "section": "3.4 Would Coordinates Help?",
    "text": "3.4 Would Coordinates Help?\nThere are other functions in the tabula software which provide the coordinates of each piece of text. How might this make it easier to ensure tables are read in correctly?\n\n\nYour answer"
  },
  {
    "objectID": "assignment-repos/05-pdf-parsing/index.html#explore-package-documentation",
    "href": "assignment-repos/05-pdf-parsing/index.html#explore-package-documentation",
    "title": "Lab: PDF Data",
    "section": "3.5 Explore Package Documentation",
    "text": "3.5 Explore Package Documentation\nFind a function that will provide coordinates for each text component, and write out the steps you might use to convert this data into a clean tabular format. What challenges will you face?\n\n\nYour answer should include an ordered list of steps (indent with at least 2 spaces to form a nested list, if necessary), as well as a response to the open-ended question at the end."
  },
  {
    "objectID": "assignment-repos/05-pdf-parsing/index.html#a-classical-problem",
    "href": "assignment-repos/05-pdf-parsing/index.html#a-classical-problem",
    "title": "Lab: PDF Data",
    "section": "3.6 A Classical Problem",
    "text": "3.6 A Classical Problem\nUsing either approach, get the salary data for all individuals in the Classics department, across all 4 reports (this should require reading in about 2 pages from each report). Plot the salaries of each individual. Generate a second plot of the total budget for the classics department, split by faculty, administration (the chair), and staff (including student workers. What do you notice?\nYour answer should address some of the following questions:\n\nHow has the budget for Classics changed over the last 15 years?\nHow has the proportional allocation of salaries to faculty, staff, and administration changed?\nWhat do you think is driving that change?\n\nYour plots must include appropriate titles and legends, and be well constructed using an appropriate mapping. Each plot should be accompanied by a 2-4 sentence description.\n\n\nCode chunk(s)\n\n\n\nOverall observations"
  },
  {
    "objectID": "assignment-repos/05-pdf-parsing/index.html#quality-control",
    "href": "assignment-repos/05-pdf-parsing/index.html#quality-control",
    "title": "Lab: PDF Data",
    "section": "3.7 Quality Control",
    "text": "3.7 Quality Control\nIf you were to process the full set of faculty salary data across all departments, what quality control measures would you use to ensure your functions functioned as expected? Explain your answer and your reasoning.\n\n\nYour answer and explanation"
  },
  {
    "objectID": "slides/01-01-syllabus.html#contact-info",
    "href": "slides/01-01-syllabus.html#contact-info",
    "title": "Syllabus",
    "section": "Contact Info",
    "text": "Contact Info\n\nEmail: susan.vanderplas@unl.edu\nOffice: 343D Hardin Hall North Wing\nOffice Hours:\nhttps://calendly.com/drvanderplas/officehours\nSelf-serve, by Zoom unless you specify otherwise."
  },
  {
    "objectID": "slides/01-01-syllabus.html#tentative-schedule",
    "href": "slides/01-01-syllabus.html#tentative-schedule",
    "title": "Syllabus",
    "section": "Tentative Schedule",
    "text": "Tentative Schedule\n\n\n\nTentative schedule of class topics & project due dates\n\n\nWeek\nTopic\nImportant Dates\n\n\n\n\n1\nIntro & HTML Primer\n\n\n\n2\nWeb Scraping\n\n\n\n3\nRecord-based Data and List Processing\n\n\n\n4\nApplication Programming Interfaces\n\n\n\n5\nPDF Tools\n\n\n\n6\nUsing Appropriate Graphics\n\n\n\n7\nDynamic Communication with Shiny\n\n\n\n8\nInteractive JavaScript Graphics\n\n\n\n9\nLinked Interactive Graphics\nScreencast Due\n\n\n10\nApproaches to Big Data\nScreencast Peer Reviews Due\n\n\n11\nSQL and SQLite\n\n\n\n12\nApache Arrow\n\n\n\n13\nDuckDB\n\n\n\n14\nThanksgiving\n\n\n\n15\nSpecial Topics\n\n\n\n16\nSpecial Topics\n\n\n\n17\nFinals\nFinal Exam"
  },
  {
    "objectID": "slides/01-01-syllabus.html#course-rhythm",
    "href": "slides/01-01-syllabus.html#course-rhythm",
    "title": "Syllabus",
    "section": "Course Rhythm",
    "text": "Course Rhythm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nS\nS\nM\nT\nW\nR\nF\n\n\n\n\n\n\nWeek before\nWeek Start\n\n\n\n\n\n\n\n\nTextbook\nSkim\nRead\nWork Examples\nReading Quiz\nAssignment-focused review\nReview difficult concepts\nSummarize important concepts\n\n\n\nHomework\n\nAccept assignment & pull\nRead assignment\nWork in class\nWork at home (if necessary)\nWork in class\nSubmit assignment"
  },
  {
    "objectID": "slides/01-01-syllabus.html#grading",
    "href": "slides/01-01-syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\n\n\n\nAssignments\nWeight\n\n\n\n\nReading Quizzes & Participation\n10%\n\n\nHomework/Labs\n50%\n\n\nScreencast\n20%\n\n\nFinal Exam\n20%"
  },
  {
    "objectID": "slides/01-01-syllabus.html#grading-1",
    "href": "slides/01-01-syllabus.html#grading-1",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\n\n\n\nLetter grade\nX +\nX\nX -\n\n\n\n\nA\n96.5\n93.5\n89.5\n\n\nB\n86.5\n83.5\n79.5\n\n\nC\n76.5\n73.5\n69.5\n\n\nD\n66.5\n63.5\n60.5\n\n\nF\n&lt;60.5"
  },
  {
    "objectID": "slides/01-01-syllabus.html#major-assignments",
    "href": "slides/01-01-syllabus.html#major-assignments",
    "title": "Syllabus",
    "section": "Major Assignments",
    "text": "Major Assignments\n\nScreencast\n\nguidelines available by Week 4\n\nFinal Exam:\n\nIn class component + take-home component\nTake-home will happen during “dead week”\nIn class portion during final exam period"
  },
  {
    "objectID": "slides/01-01-syllabus.html#ai-use-policy",
    "href": "slides/01-01-syllabus.html#ai-use-policy",
    "title": "Syllabus",
    "section": "AI Use Policy",
    "text": "AI Use Policy\n\nUsing AI to write code (Don’t!)\nUsing AI to explain an error (Ok)\nUsing AI to write explanations (Don’t)\nIf you use AI\n\nDocument the use, version, input, and output\nProvide a diff of your submission vs. the AI output"
  },
  {
    "objectID": "slides/01-01-syllabus.html#oral-exams",
    "href": "slides/01-01-syllabus.html#oral-exams",
    "title": "Syllabus",
    "section": "Oral Exams",
    "text": "Oral Exams\n\nI reserve the right to replace any grade with an oral exam on your submission\nAllows me to clarify how much you understand when that isn’t clear\nHelps when I write an ambiguous question or you miss a key part of a question\nIf you can’t explain your code/answer you don’t get any credit"
  }
]