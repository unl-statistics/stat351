---
title: "HTML, XML, CSS, and XPath"
subtitle: "The Building Blocks of the Internet"
echo: true
format: revealjs
---

## Why?
::: incremental
- So much data is online!
- But, ...
    - Manually copying information is error prone
    - Stuff is updated in real time
    - I'm lazy

- We can automate gathering data using scripts
:::

## Examples {.smaller}

- Gather the price of something every morning at 8am to determine whether it's cheap enough to buy

- Estimate what you could sell your house for, given recent sales of comparable houses in the same city

- Create a database of adoptable pets in your area (with pics) and determine how long it takes for different pets to be adopted

- Assemble a database of profiles on dating sites to determine whether gender and sexual orientation is related to emoticon/emoji use

- Assemble a directory of contact information for all faculty at UNL to conduct an unofficial faculty survey

## XML

- relatively common data storage format
- Fields are delimited by tags `<tagName attr1=value1 ...>`. All tags are closed with `</tagName>`

- Tags may contain attribute-value pairs

- Tags may have children nested between `<tagName>` and `</tagName>`

- Tags may also contain "content" between the tags -- plain text information


## XML Terms {.smaller}

```
<family name="Vanderplas">
    <person given-name="Susan">Mother</person>
    <person given-name="Ryan">Father</person>
    <person given-name="Alex" nickname='Bug'>Son</person>
    <person given-name="Zoey" nickname='Zozo, Lovebug'>Daughter</person>
    <pet type="dog" given-name="Edison" nickname="Eddie">Security detail, Cleanup crew</pet>
    <pet type="dog" given-name="Ivy" nickname="Flufferina, Q-tip">Snuggle agent, Cleanup crew, Comic relief</pet>
</family>
```

- `given-name` and `family-name` are **attributes** with values for each person and pet. `nickname` is an attribute, but is not present for all nodes

- `<person>...</person>` and `<pet>...</pet>` are **child nodes** of `<family></family>`

- The content of each child node is the entity's role in the family

- XML data is **nested** and does not always translate to tabular form easily

## HTML vs. XML

::: columns
::: column
### HTML

- Tags **display** information
- Tags are pre-defined
- Tags aren't always closed    
[`<br/>`, `<img/>`]{.smaller}
- Not case-sensitive
- Ignores white-space
:::
::: column
### XML

- Tags **describe** information
- Data schema defines tags
- Tags must be closed    
&nbsp;
- Case-sensitive
- May ignore white space
:::
:::


## Your Turn: Web Page Anatomy {.smaller .inverse}

1. Open [the textbook chapter](https://srvanderplas.github.io/stat-computing-r-python/part-advanced-topics/03-web-scraping.html)
2. Access Developer Tools for your browser
    - right-click + select "Inspect" 
    - OR, Ctrl/Cmd + J

3. Find the following elements. What attributes and content do they have?
    - Document type declaration
    - `<html>` node
    - `<head>` and `<body>` nodes
    - `<h2>`, `<h3>`, `<h4>` and `<p>` nodes
    - `<table>`, `<tr>`, `<th>`, and `<td>` nodes
    - `<a>` node(s)
    
## Selecting Nodes (CSS)

- [SelectorGadget](https://selectorgadget.com/) extension can be helpful

- `.xxx` = "has class xxx"
- `#xxx` = "has ID xxx"
- `xxx` = "node xxx"
- `xxx yyy` = "node yyy, a  descendant of xxx"
- `xxx > yyy` = "node yyy, a direct descendant of xxx"

## Your Turn: CSS Selectors {.inverse}

Construct a CSS selector that will get all mathematicians from [this list](https://en.wikipedia.org/wiki/List_of_mathematicians_born_in_the_19th_century) without any extra links.


# Reading and Working with HTML files in R and Python

## Common Libraries 

::: columns
::: {.column .r-fit-text}
### R

- `rvest` - higher-level, process HTML 
  - Mostly built around `xml2`
- `xml2` - lower-level, navigate/extract HTML elements

- `httr2` - higher-level, talk to web servers
- `curl` - lower-level, talk to web servers

- `chromote` - remote control a browser

:::

:::  {.column .r-fit-text}
### Python

- `bs4` (Beautiful Soup) - processing HTML
- `urllib` - work with URLs (combining, parsing)
- `requests` - talk to web servers
- `selenium` - remote control a browser
:::
:::

## Strategy: Set a User Agent

::: panel-tabset
### R {-}
```{r}
library(xml2)
library(rvest)
library(purrr)
library(dplyr)
library(tibble)

url<-"https://en.wikipedia.org/wiki/List_of_mathematicians_born_in_the_19th_century"

read_html(url)
```

### Python {-}

```{python}
#| error: true
from bs4 import BeautifulSoup, SoupStrainer
import requests
import pandas as pd

url = "https://en.wikipedia.org/wiki/List_of_mathematicians_born_in_the_19th_century"
# If at first you don't succeed, 
# try adding a user-agent
headers = {'user-agent':'python-data-programming, svanderplas2-at-unl-dot-edu'}
req = requests.get(url, headers=headers)
page = req.content
page
```

:::

## Accessing Specific Nodes

::: panel-tabset

### R {-}

```{r}
ppl <- read_html(url) |>
    html_nodes(".mw-body-content ul li")
ppl[[1]]
```

### Python {-}

First use bs4.BeautifulSoup to make the HTML easy to work with, then find the nodes we care about.

```{python}
soup = BeautifulSoup(page)
ppl = soup.select(".mw-body-content ul li")
ppl[0]
```

:::

## Strategy: Error Handling


:::: columns
::: column
::: panel-tabset

### R {-}

```{r}
try_na <- function(i, fn, ...) {
    res <- try(fn(i, ...))
    if( "try-error" %in% class(res)) {
        res <- NA
    }
    if(length(res) == 0) {
        res <- NA
    }
    res
}
```

### Python {-}

```{python}
def try_na(x, expression):
  # If x is NA, then the result must also be NA
  # for most HTML-parsing expressions... NOT FOOLPROOF
  if pd.isna(x):
    return pd.NA
  else:
    try:
      res = eval(expression, {}, {"x": x})
    except:
      return pd.NA
    if res is None: # Tests for an empty return value
      return pd.NA
    if len(res) == 0:
      return pd.NA
  return res

```

:::
:::

::: column

Not all HTML nodes have the same attributes/children.

Preemptive error handling can be helpful.

`try_na()` will return

- the value if one exists, 
- NA if the command results in an error
- NA if the result has 0 length

:::
::::

## Strategy: Use Functions {.scrollable}
[(like, all the time)]{.emph .cerulean}

::: panel-tabset
### R {-}

```{r}
math_ppl <- tibble(
    content = html_text(ppl),
    link_info = map(ppl, ~try_na(., fn = html_children)),
    name = map_chr(link_info, ~try_na(., fn = html_text)),
    name2 = map_chr(link_info, ~try_na(., fn = html_attr, "title")),
    link = map_chr(link_info, ~try_na(., fn = html_attr, "href"))
) |>
    select(-link_info)
head(math_ppl)
```

### Python {-}

```{python}
content = [try_na(i, "x.text") for i in ppl]
link_info = [try_na(i, "x.find('a')") for i in ppl]
name = [try_na(i, 'x.text') for i in link_info]
name2 = [try_na(i, 'x.attrs["title"]') for i in link_info]
link = [try_na(i, 'x.attrs["href"]') for i in link_info]
math_ppl = pd.DataFrame({'content': content, 'name': name, 'name2': name2, 'link': link})

math_ppl.head()
```

:::


## Know your HTTP error codes

<iframe width="850" height="500" src="https://http.cat/" title="HTTP Cats"></iframe>



# How JavaScript Ruined the Internet

## Once Upon a Time

We wrote webpages the hard way...

![](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExb3p3Y2R4ZW56cm93N3pieTRydm12dzVxNjZxamUwc3cxb3RwbDM0MyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/11XcgX9MWV3a8M/giphy.gif){fig-alt="Moses, carrying three stone tablets, says 'The lord Jehovah has given unto you these fifteen...'. He drops one and says 'Oy.'"}


## Once Upon a Time

::: columns
::: column
```
<!DOCTYPE html>
<head></head>
<body>
<h1>Susan's webpage</h1>
<p>My favorite things...</p>
<ul>
<li>Rainbows</li>
<li>Puppies</li>
<li>Frosting</li>
</ul>
</body>
</html>
```
::: 
::: column

![](images/1999-html.png){fig-alt="A screenshot of the rendered website using the code on the left"}

[Link](images/1999.html)
:::
:::

## Ooh, we can style stuff!

::: columns
::: column
```
<!DOCTYPE html>
<head> <style>
body{ background-image: url('rainbows.jpg');}
</style> </head>
<body>
<h1 style="color: #ffffff;">Susan's webpage</h1>
<p style="color: #ffffff;">My favorite things...</p>
<ul  style="color: #ffffff;">
<li>Rainbows</li><li>Puppies</li>
<li>Frosting</li></ul>
</body>
</html>
```
:::
::: column

![](images/2000-html.png){fig-alt="A screenshot of the rendered website using the code on the left"}

[Link](images/2000.html)
:::
:::

## CSS to simplify styling

::: columns
::: column

```
<html>
<head>
<link rel="stylesheet" href="mystyles.css"/>
</head>
<body>
<h1>Susan's webpage</h1>
<p>My favorite things...</p>
<ul><li>Rainbows</li><li>Puppies</li><li>Frosting</li></ul>
</body>
</html>
```
[CSS was actually introduced in 1996, but it took a while to catch on.]{.emph .purple}
:::
::: column

![](images/2002-html.png){fig-alt="A screenshot of the rendered website using the code on the left"}

[Link](images/2002.html)
:::
:::

## Then came JavaScript!

::: columns
::: column
```
<html>
<head>
<link rel="stylesheet" href="mystyles.css"/>
</head>
<body>
<script src="test.js"></script>
<h1>Susan's webpage</h1>
<p>My favorite things...</p>
<ul><li>Rainbows</li><li>Puppies</li><li>Frosting</li></ul>
</body>
</html>
```


We were really annoying!

:::
::: column
![](images/2004-html.gif){fig-alt="A gif stepping through the javascript pop-ups which say 'hello, my name is Susan', 'This is my website', 'What is your name? A: John Doe', 'Hello John Doe!', before proceeding to the main site page."}

:::
:::

## Fast Forward 15 years

Javascript is used to

- add new elements to HTML
- provide animations
- create graphics
- actually process information and write data to servers


It's a full-fledged programming language!


## Zeta Tech Jobs - ðŸ˜ˆ JS

::: columns
::: column

[Site](https://www.zeta.tech/in/careers/work-with-us/?department=dataanalytics)

```{r}
library(httr)
url <- "https://www.zeta.tech/in/careers/work-with-us/?department=dataanalytics"
ua <- "Mozilla/5.0 (platform; rv:gecko-version) Gecko/gecko-trail Firefox/firefox-version"
page <- GET(url, user_agent(ua)) |>
  read_html()

page |> html_nodes(".link")
```
:::
::: column
![](images/zeta-jobs-demo.png){fig-alt="A screenshot of the Zeta jobs site with selectorgadget activated, showing that .link selects 118 objects within the page, in contrast to the 0 found with html_nodes."}
:::
:::

[[Original Source of this Example](https://www.reddit.com/r/learnpython/comments/16k5itz/scraping_dynamic_site_with_python/)]{.bottom}

## Options for Dynamic Pages

1. Find a different source for the data

2. See if there's an API call under the hood

3. Use browser emulation via selenium/chromote


